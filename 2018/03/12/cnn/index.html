<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="关于前端与设计、黑客与画家 | 怀亮，Web & Mobile Lover，Software Engineer，Game Designer | 这里是 @Huailiang怀亮 的个人博客，与你一起发现更大的世界。">
    <meta name="keywords"  content="怀亮, Huailiang怀亮, Huailiang, Unity, Tensorflow, AR, 怀亮的博客, Huailiang Blog, 博客, U3D, 互联网, AR, JavaScript">
    <meta name="theme-color" content="#000000">

    <title>TensorFlow-搭建 CNN 网络 - 怀亮的博客 | Huailiang Blog</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">

    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2018/03/12/cnn/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Huailiang Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/portfolio/">Portfolio</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from
     * $toggle/$collapse will break global delegation.
     *
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-os-metro.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/post-bg-os-metro.jpg')
    }

    
    header.intro-header .header-mask{
        width: 100%;
        height: 100%;
        position: absolute;
        background: rgba(0,0,0, 0.3);
    }
    
</style>
<header class="intro-header" >
    <div class="header-mask"></div>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                        <a class="tag" href="/tags/#人工智能" title="人工智能">人工智能</a>
                        
                        <a class="tag" href="/tags/#Tensorflow" title="Tensorflow">Tensorflow</a>
                        
                    </div>
                    <h1>TensorFlow-搭建 CNN 网络</h1>
                    
                    
                    <h2 class="subheading">主流的图像、语音识别算法</h2>
                    
                    <span class="meta">Posted by Huailiang on March 12, 2018</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<blockquote>
  <p>人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。</p>
</blockquote>

<p>CNN（Convolutional Neural Network）——卷积神经网络，人工神经网络（Neural Network，NN）的一种，其它还有RNN、DNN等类型，而CNN就是利用卷积进行滤波的神经网络。换句话说，CNN就是卷积加神经网络。</p>

<p>卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是非全连接的， 另一方面同一层中某些神经元之间的连接的权重是共享的（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物 神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。现在 以CNN 神经网络模型的主要用来图像归类、语音识别。</p>

<p><img src="/img/in-post/post-tf/cnn02.png" alt="" /></p>

<p>主流的CNN 神经网络主要包含以下几个步骤： 输入(in)-&gt;卷积(conv2d)-&gt;池化（pooling）-&gt;全连接（Fully Connect）-&gt;归类（softmax）-&gt;输出（out）</p>

<p>下面以 Python 语言展示cnn 的 Graph 生成过程。</p>

<ul>
  <li>定义 W 权重和 biases</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span>  <span class="nf">var_weight</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
	<span class="c"># tf.truncated_normal(shape, mean, stddev)</span>
    <span class="c"># shape表示生成张量的维度，mean是均值，stddev是标准差</span>
	<span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">var_bias</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">))</span>

</code></pre></div></div>

<h3 id="卷积-conv2d-与池化-pooling-操作">卷积 conv2d 与池化 pooling 操作</h3>

<p>首先，关于什么是卷积，文章（3）有一牛人用一句话总结得很好：卷积就是带权的积分，看下面的一维卷积公式:</p>

<p><img src="/img/in-post/post-tf/cnn03.png" alt="" /></p>

<p>函数值与权值的乘积相加即可得到卷积值c(x)，换句话说，我们对函数值的加权叠加，即可得到x处的卷积值。CNN利用的是多维卷积，但原理一样，不多说了，同时建议不要过多纠缠这个概念，理解就好，以后若作科研再作深入了解。对于图像处理，CNN会对输入图像矩阵化，然后从矩阵第一个元素开始逐一进行卷积运算。</p>

<p>池化层存在的目的是缩小输入的特征图，简化网络计算复杂度；同时进行特征压缩，突出主要特征。简言之，即取区域平均或最大，如下图所示</p>

<p><img src="/img/in-post/post-tf/cnn01.jpg" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c"># 卷积操作</span>
<span class="k">def</span> <span class="nf">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">):</span>
  <span class="s">"""
   卷积遍历各方向步数为1，SAME：边缘外自动补0，遍历相乘
  """</span>
	<span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">,[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s">"SAME"</span><span class="p">)</span>

<span class="c"># 池化操作</span>
<span class="k">def</span> <span class="nf">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="s">"""
  图像长宽压缩，过滤无用的信息
  池化层采用kernel大小为2*2，步数也为2，周围补0，取最大值。数据量缩小了4倍  
  """</span>
	<span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">ksize</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">padding</span><span class="o">=</span><span class="s">'SAME'</span><span class="p">)</span>


</code></pre></div></div>
<p>padding=’SAME’ 表示输出大小不会发生改变，filter 以0填充
 padding=’VILID’ 信息会有损 一般在池化pooling才会改变大小</p>

<ul>
  <li>定义模型输入 这里从 MNIST 测试数据获取</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xs</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,[</span><span class="bp">None</span><span class="p">,</span><span class="mi">784</span><span class="p">])</span><span class="o">/</span><span class="mi">255</span>
<span class="n">ys</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,[</span><span class="bp">None</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">keep_prob</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="第一层神经网络-以5x5的-过滤器filter">第一层神经网络 以5x5的 过滤器（filter）</h3>

<p>输出深度为32的卷积池化操作, 卷积之后得到的大小是28x28x32,池化之后得到的大小是14x14x32</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">"""
第一二参数值得卷积核尺寸大小，即patch，第三个参数是图像通道数，第四个参数是卷积核的数目，代表会出现多少个卷积特征图像
"""</span>
<span class="n">W_conv1</span> <span class="o">=</span> <span class="n">var_weight</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">32</span><span class="p">])</span>

<span class="n">b_conv1</span> <span class="o">=</span> <span class="n">var_bias</span><span class="p">([</span><span class="mi">32</span><span class="p">])</span>
<span class="c"># 图片乘以卷积核，并加上偏执量，卷积结果28x28x32  </span>
<span class="n">h_conv1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x_image</span><span class="p">,</span> <span class="n">W_conv1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_conv1</span><span class="p">)</span>

<span class="n">h_pool1</span> <span class="o">=</span> <span class="n">maxpool</span><span class="p">(</span><span class="n">h_conv1</span><span class="p">)</span>     <span class="c"># output size 14x14x32</span>

</code></pre></div></div>

<p>激活函数层：它的作用前面已经说了，这里讲一下代码中采用的relu（Rectified Linear Units，修正线性单元）函数，它的数学形式如下：</p>

<p>ƒ(x) = max(0, x)</p>

<p>这个函数非常简单，其输出一目了然，小于0的输入，输出全部为0，大于0的则输入与输出相等。该函数的优点是收敛速度快，除了它，keras库还支持其它几种激活函数，如下：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>softplus
softsign
tanh
sigmoid
hard_sigmoid
linear
</code></pre></div></div>

<p>它们的函数式、优缺点度娘会告诉你，不多说。对于不同的需求，我们可以选择不同的激活函数，这也是模型训练可调整的一部分，运用之妙，存乎一心，请自忖之。</p>

<h3 id="第二层继续卷积池化操作">第二层继续卷积池化操作</h3>

<p>使图像继续长宽变小深度加厚,第二层输出的大小事7x7x64</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## conv2 layer ##</span>
<span class="n">W_conv2</span> <span class="o">=</span> <span class="n">var_weight</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span> <span class="c"># patch 5x5, in size 32, out size 64</span>
<span class="n">b_conv2</span> <span class="o">=</span> <span class="n">var_bias</span><span class="p">([</span><span class="mi">64</span><span class="p">])</span>
<span class="n">h_conv2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">conv2d</span><span class="p">(</span><span class="n">h_pool1</span><span class="p">,</span> <span class="n">W_conv2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_conv2</span><span class="p">)</span> <span class="c"># output size 14x14x64</span>
<span class="n">h_pool2</span> <span class="o">=</span> <span class="n">max_pool</span><span class="p">(</span><span class="n">h_conv2</span><span class="p">)</span>     <span class="c"># output size 7x7x64</span>

</code></pre></div></div>

<h3 id="第三层-全连接层dense-layer">第三层 全连接层（dense layer）</h3>

<p>全连接层的作用就是用于分类或回归，对于我们来说就是分类。keras将全连接层定义为Dense层，其含义就是这里的神经元连接非常“稠密”。我们通过Dense()函数定义全连接层。这个函数的一个必填参数就是神经元个数，其实就是指定该层有多少个输出。在我们的代码中，第一个全连接层（#14 Dense层）指定了512个神经元，也就是保留了512个特征输出到下一层。这个参数可以根据实际训练情况进行调整，依然是没有可参考的调整标准，自调之。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W_fc1</span> <span class="o">=</span> <span class="n">var_weight</span><span class="p">([</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1024</span><span class="p">])</span>
<span class="n">b_fc1</span> <span class="o">=</span> <span class="n">var_bias</span><span class="p">([</span><span class="mi">1024</span><span class="p">])</span>
<span class="c"># [n_samples, 7, 7, 64] -&gt;&gt; [n_samples, 7*7*64]</span>
<span class="n">h_pool2_flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">h_pool2</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">64</span><span class="p">])</span>
<span class="n">h_fc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_pool2_flat</span><span class="p">,</span> <span class="n">W_fc1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_fc1</span><span class="p">)</span>
<span class="n">h_fc1_drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">h_fc1</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="第四层-正则化-归类">第四层 正则化-归类</h3>

<p>数据集先浮点后归一化的目的是提升网络收敛速度，减少训练时间，同时适应值域在（0,1）之间的激活函数，增大区分度。其实归一化有一个特别重要的原因是确保特征值权重一致。举个例子，我们使用mse这样的均方误差函数时，大的特征数值比如(5000-1000)2与小的特征值(3-1)2相加再求平均得到的误差值，显然大值对误差值的影响最大，但大部分情况下，特征值的权重应该是一样的，只是因为单位不同才导致数值相差甚大。因此，我们提前对特征数据做归一化处理，以解决此类问题。关于归一化的详细介绍有兴趣的请参考如下链接：<a href="https://www.thinksaas.cn/group/topic/491257/">深入理解CNN细节之数据预处理</a></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## fc2 layer ##</span>
<span class="n">W_fc2</span> <span class="o">=</span> <span class="n">var_weight</span><span class="p">([</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">b_fc2</span> <span class="o">=</span> <span class="n">var_bias</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_fc1_drop</span><span class="p">,</span> <span class="n">W_fc2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_fc2</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>
    <p>计算结果</p>

    <p>用交叉熵计算损失，优化器迭代，每隔50步打印准确度</p>
  </li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># the error between prediction and real data</span>
<span class="c"># 定义交叉熵为loss函数    </span>
<span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">ys</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prediction</span><span class="p">),</span><span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>      <span class="c"># loss</span>
<span class="c"># 调用优化器优化，其实就是通过喂数据争取cross_entropy最小化    </span>
<span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
	<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
	<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>

	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
	    <span class="n">batch_xs</span><span class="p">,</span> <span class="n">batch_ys</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
	    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">xs</span><span class="p">:</span> <span class="n">batch_xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">:</span> <span class="n">batch_ys</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
	    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
	        <span class="k">print</span><span class="p">(</span><span class="n">compute_accuracy</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">[:</span><span class="mi">1000</span><span class="p">],</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]))</span>

</code></pre></div></div>

<p>运行结果：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
2018-03-12 13:40:29.201888: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
0.054
0.767
0.866
0.895
0.905
0.925
0.928
0.931
0.938
0.943
0.943
0.951
0.951
0.961
0.962
0.96
0.967
0.963
0.967
0.966
[Finished in 218.8s]
</code></pre></div></div>

<p>从最终结果可以看出，运行的准确率可以达到96.6%，注意编译运行的时候负载特别高</p>

<p><img src="/img/in-post/post-tf/cpu.jpeg" alt="" /></p>

<p>通过上面的案例，我们再回到文章开篇提出的问题。加入我们知道小明数次选择，假使小明我们的考虑的因素是固定不变的而且外部环境没有发生变化，下次要不要出门，我们就能求出概率啦。哈哈。。。</p>



                <hr style="visibility: hidden;">

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2018/03/11/urlib/" data-toggle="tooltip" data-placement="top" title="Python扒取网络图片">
                        Previous<br>
                        <span>Python扒取网络图片</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2018/03/13/tfrecord/" data-toggle="tooltip" data-placement="top" title="TensorFlow TFRecord图形处理工具">
                        Next<br>
                        <span>TensorFlow TFRecord图形处理工具</span>
                        </a>
                    </li>
                    
                </ul>


                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

                
            </div>

    <!-- Side Catalog Container -->
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				          
                				       <a href="/tags/#OSX" title="OSX" rel="4">OSX</a>
        				          
                				       <a href="/tags/#前端开发" title="前端开发" rel="15">前端开发</a>
        				          
                				       <a href="/tags/#JavaScript" title="JavaScript" rel="8">JavaScript</a>
        				          
                				       <a href="/tags/#Lua" title="Lua" rel="1">Lua</a>
        				          
                				       <a href="/tags/#Unity" title="Unity" rel="16">Unity</a>
        				          
                				       <a href="/tags/#手机游戏" title="手机游戏" rel="7">手机游戏</a>
        				          
                				       <a href="/tags/#Jenkins" title="Jenkins" rel="1">Jenkins</a>
        				          
                				       <a href="/tags/#C++" title="C++" rel="3">C++</a>
        				          
                				       <a href="/tags/#Tensorflow" title="Tensorflow" rel="6">Tensorflow</a>
        				          
                				       <a href="/tags/#人工智能" title="人工智能" rel="7">人工智能</a>
        				          
                				       <a href="/tags/#Python" title="Python" rel="8">Python</a>
        				          
                				       <a href="/tags/#工具" title="工具" rel="5">工具</a>
        				          
                				       <a href="/tags/#强化学习" title="强化学习" rel="4">强化学习</a>
        				          
        			      </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="http://www.gamelook.com.cn">GameLook</a></li>
                    
                        <li><a href="http://forum.china.unity3d.com/forum.php">游戏人社区</a></li>
                    
                        <li><a href="https://unity3d.com">Unity3D</a></li>
                    
                        <li><a href="https://www.uwa4d.com">UWA-简单优化</a></li>
                    
                        <li><a href="https://blog.openai.com/openai-baselines-ppo/">OpenAI新的策略优化算法PPO</a></li>
                    
                        <li><a href="https://www.zhihu.com/people/morvan/activities">知乎-莫烦 Python</a></li>
                    
                        <li><a href="http://blog.sina.com.cn/s/blog_471132920101d5kh.html">风宇冲的 Blog</a></li>
                    
                        <li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_beginners.html">极客学院-Tensotflow</a></li>
                    
                        <li><a href="https://unity3d.com/cn/solutions/mobile-ar">Unity for Mobile AR</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>






<!-- disqus 公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "Huailiang";
    var disqus_identifier = "/2018/03/12/cnn";
    var disqus_url = "http://localhost:4000/2018/03/12/cnn/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus 公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                     

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                     
                    <li>
                        <a target="_blank" href="https://weibo.com/u/6212299692">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                     
                    <li>
                        <a target="_blank" href="https://www.facebook.com/profile.php?id=100004290725320">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                     
                    <li>
                        <a target="_blank" href="https://github.com/huailiang">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                     
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Huailiang Blog 2018
                    <br> Power by
                    <a href="http://huailiang.github.io">Huailiang</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="100px" height="20px" src="https://ghbtns.com/github-btn.html?user=huailiang&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src=" /js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src=" /js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src=" /js/hux-blog.min.js "></script>

<!-- Service Worker -->

<script src=" /js/snackbar.js "></script>
<script src=" /js/sw-registration.js "></script> 

<!-- async load function -->
<script>
    function async(u, c) {
        var d = document, t = 'script',
            o = d.createElement(t),
            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
        s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if ($('#tag_cloud').length !== 0) {
        async('/js/jquery.tagcloud.js', function () {
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: { start: '#bbbbee', end: '#0085a1' },
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function () {
        var $nav = document.querySelector("nav");
        if ($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-49627206-1';
    var _gaDomain = 'github.io';

    // Originial
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script> 


<!-- Baidu Tongji -->



<!-- Side Catalog -->



<!-- Multi-Lingual -->



<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
