<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="关于前端与设计、黑客与画家 | 怀亮，Web & Mobile Lover，Software Engineer，Game Designer | 这里是 @Huailiang怀亮 的个人博客，与你一起发现更大的世界。">
    <meta name="keywords"  content="怀亮, Huailiang怀亮, Huailiang, Unity, Tensorflow, AR, 怀亮的博客, Huailiang Blog, 博客, U3D, 互联网, AR, JavaScript">
    <meta name="theme-color" content="#000000">

    <title>TensorFlow 学习入门 - 怀亮的博客 | Huailiang Blog</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">

    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2018/03/08/tensor/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Huailiang Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
                    
                    <li>
                        <a href="/about/">About</a>
                    </li>
                    
                    <li>
                        <a href="/portfolio/">Portfolio</a>
                    </li>
                    
                    <li>
                        <a href="/tags/">Tags</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    var __HuxNav__ = {
        close: function(){
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        },
        open: function(){
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }

    // Bind Event
    $toggle.addEventListener('click', function(e){
        if ($navbar.className.indexOf('in') > 0) {
            __HuxNav__.close()
        }else{
            __HuxNav__.open()
        }
    })

    /**
     * Since Fastclick is used to delegate 'touchstart' globally
     * to hack 300ms delay in iOS by performing a fake 'click',
     * Using 'e.stopPropagation' to stop 'touchstart' event from
     * $toggle/$collapse will break global delegation.
     *
     * Instead, we use a 'e.target' filter to prevent handler
     * added to document close HuxNav.
     *
     * Also, we use 'click' instead of 'touchstart' as compromise
     */
    document.addEventListener('click', function(e){
        if(e.target == $toggle) return;
        if(e.target.className == 'icon-bar') return;
        __HuxNav__.close();
    })
</script>


    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/home-bg-art.jpg" width="0" height="0"> -->

<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/home-bg-art.jpg')
    }

    
    header.intro-header .header-mask{
        width: 100%;
        height: 100%;
        position: absolute;
        background: rgba(0,0,0, 0.3);
    }
    
</style>
<header class="intro-header" >
    <div class="header-mask"></div>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                        <a class="tag" href="/tags/#人工智能" title="人工智能">人工智能</a>
                        
                        <a class="tag" href="/tags/#Tensorflow" title="Tensorflow">Tensorflow</a>
                        
                    </div>
                    <h1>TensorFlow 学习入门</h1>
                    
                    
                    <h2 class="subheading">使用TensorFlow Python API 编程入门</h2>
                    
                    <span class="meta">Posted by Huailiang on March 8, 2018</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<blockquote>
  <p>TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统，其命名来源于本身的运行原理。Tensor（张量）意味着N维数组，Flow（流）意味着基于数据流图的计算，TensorFlow为张量从流图的一端流动到另一端计算过程。TensorFlow是将复杂的数据结构传输至人工智能神经网中进行分析和处理过程的系统。</p>
</blockquote>

<h2 id="导入tensorflow">导入TensorFlow</h2>

<p>典型的导入TensorFlow程序的做法如下：</p>

<pre><code class="language-Python">import tensorflow as tf
</code></pre>

<p>运行这个，可以得到Python下的TensorFlow中的所有类、方法和符号。大多数的文档默认你已经执行了这个。</p>

<h2 id="计算图-computational-graph">计算图 (Computational Graph)</h2>

<p>TensorFlow Core程序是由两个分离的部分组成的：</p>

<p>1、构建计算图。</p>

<p>2、运行计算图。</p>

<p>一个计算图是一系列TensorFlow操作的点(nodes)。我们构建一个简单的计算图。每一个点需要零个或者更多的张量(tensor)作为输入，并且产生一个张量作为输出。</p>

<h3 id="数据类型">数据类型</h3>

<p>下图列出了所有的 tensorflow的数据类型</p>

<p><img src="/img/in-post/post-tf/tf01.jpeg" alt="" /></p>

<h3 id="常量constant">常量(Constant)</h3>

<p>点的一种形式是常数。像所有TensorFlow常数，它不需要输入，而它会从内部输出一个值。我们可以创建两个点的张量c1和c2，如下所示：</p>

<pre><code class="language-Python">import tensorflow as tf
import numpy as np

c1 = tf.constant([0.3,2.0],tf.float32)
c2 = tf.constant([1.3,1.2],tf.float32)
print c1,c2

"""
 输出：Tensor("Const:0", shape=(2,), dtype=float32) Tensor("Const_1:0", shape=(2,), dtype=float32)
"""
</code></pre>

<p>注意到，它输出的并不是[0.3,2.0]和([1.3,1.2]的值，而是对应的点张量，要通过开启会话，才能直接输出其值。这直接反映了TensorFlow的构建和计算是分离的。</p>

<h3 id="张量的阶形状">张量的阶、形状</h3>

<p>TensorFlow用张量这种数据结构来表示所有的数据.你可以把一个张量想象成一个n维的数组或列表.一个张量有一个静态类型和动态类型的维数.张量可以在图中的节点之间流通.</p>

<p>阶
在TensorFlow系统中，张量的维数来被描述为阶.但是张量的阶和矩阵的阶并不是同一个概念.张量的阶（有时是关于如顺序或度数或者是n维）是张量维数的一个数量描述.比如，下面的张量（使用Python中list定义的）就是2阶.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    t = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
</code></pre></div></div>

<p>你可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量.对于一个二阶张量你可以用语句t[i, j]来访问其中的任何元素.而对于三阶张量你可以用’t[i, j, k]’来访问其中的任何元素.</p>

<p>阶	数学实例	Python 例子</p>

<p>0	纯量 (只有大小)	s = 483</p>

<p>1	向量(大小和方向)	v = [1.1, 2.2, 3.3]</p>

<p>2	矩阵(数据表)	m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</p>

<p>3	3阶张量 (数据立体)	t = [[[2], [4], [6]], [[8], [10], [12]], [[14], [16], [18]]]</p>

<p>n	n阶 (自己想想看)	….</p>

<p>shape [2,3] 表示为数组的意思是第一维有两个元素，第二维有三个元素，如: [[1,2,3],[4,5,6]]</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># 2-D tensor `a`</span>
 <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="o">=&gt;</span> <span class="p">[[</span><span class="mf">1.</span> <span class="mf">2.</span> <span class="mf">3.</span><span class="p">]</span>
                                                       <span class="p">[</span><span class="mf">4.</span> <span class="mf">5.</span> <span class="mf">6.</span><span class="p">]]</span>
 <span class="c"># 2-D tensor `b`</span>
 <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="o">=&gt;</span> <span class="p">[[</span><span class="mf">7.</span> <span class="mf">8.</span><span class="p">]</span>
                                                          <span class="p">[</span><span class="mf">9.</span> <span class="mf">10.</span><span class="p">]</span>
                                                          <span class="p">[</span><span class="mf">11.</span> <span class="mf">12.</span><span class="p">]]</span>
 <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">[[</span><span class="mi">58</span> <span class="mi">64</span><span class="p">]</span>
                         <span class="p">[</span><span class="mi">139</span> <span class="mi">154</span><span class="p">]]</span>


 <span class="c"># 3-D tensor `a`</span>
 <span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">13</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="o">=&gt;</span> <span class="p">[[[</span> <span class="mf">1.</span>  <span class="mf">2.</span>  <span class="mf">3.</span><span class="p">]</span>
                                                        <span class="p">[</span> <span class="mf">4.</span>  <span class="mf">5.</span>  <span class="mf">6.</span><span class="p">]],</span>
                                                       <span class="p">[[</span> <span class="mf">7.</span>  <span class="mf">8.</span>  <span class="mf">9.</span><span class="p">]</span>
                                                        <span class="p">[</span><span class="mf">10.</span> <span class="mf">11.</span> <span class="mf">12.</span><span class="p">]]]</span>

 <span class="c"># 3-D tensor `b`</span>
 <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span><span class="mi">25</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="o">=&gt;</span> <span class="p">[[[</span><span class="mf">13.</span> <span class="mf">14.</span><span class="p">]</span>
                                                         <span class="p">[</span><span class="mf">15.</span> <span class="mf">16.</span><span class="p">]</span>
                                                         <span class="p">[</span><span class="mf">17.</span> <span class="mf">18.</span><span class="p">]],</span>
                                                        <span class="p">[[</span><span class="mf">19.</span> <span class="mf">20.</span><span class="p">]</span>
                                                         <span class="p">[</span><span class="mf">21.</span> <span class="mf">22.</span><span class="p">]</span>
                                                         <span class="p">[</span><span class="mf">23.</span> <span class="mf">24.</span><span class="p">]]]</span>
 <span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">[[[</span> <span class="mi">94</span> <span class="mi">100</span><span class="p">]</span>
                          <span class="p">[</span><span class="mi">229</span> <span class="mi">244</span><span class="p">]],</span>
                         <span class="p">[[</span><span class="mi">508</span> <span class="mi">532</span><span class="p">]</span>
                          <span class="p">[</span><span class="mi">697</span> <span class="mi">730</span><span class="p">]]]</span>

</code></pre></div></div>

<p>tensorflow中有一类在tensor的某一维度上求值的函数，如：</p>

<p>求最大值tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None)</p>

<p>求平均值tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)</p>

<p>参数（1）input_tensor:待求值的tensor。</p>

<p>参数（2）reduction_indices:在哪一维上求解。</p>

<p>参数（3）（4）可忽略</p>

<p>举例说明：</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span><span class="o">=</span><span class="p">[[</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">],[</span><span class="mf">3.</span><span class="p">,</span><span class="mf">4.</span><span class="p">]]</span>
<span class="n">x_</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
	<span class="n">y1_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span>
	<span class="n">y2_</span> <span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
	<span class="n">y3_</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
	<span class="k">print</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y1_</span><span class="p">),</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y2_</span><span class="p">),</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y3_</span><span class="p">)</span>
	<span class="n">y1_</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span>
	<span class="n">y2_</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
	<span class="n">y3_</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
	<span class="k">print</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y1_</span><span class="p">),</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y2_</span><span class="p">),</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">y3_</span><span class="p">)</span>

</code></pre></div></div>

<p>首先求平均值，
tf.reduce_mean(x_) ==&gt; 2.5 #如果不指定第二个参数，那么就在所有的元素中取平均值</p>

<p>tf.reduce_mean(x_, 0) ==&gt; [2.,  3.] #指定第二个参数为0，则第一维的元素取平均值，即每一列求平均值</p>

<p>tf.reduce_mean(x_, 1) ==&gt; [1.5,  3.5] #
指定第二个参数为1，则第二维的元素取平均值，即每一行求平均值</p>

<p>同理，还可用tf.reduce_max()求最大值。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#输出结果：
2.5 [2. 3.] [1.5 3.5]
4.0 [3. 4.] [2. 4.]
</code></pre></div></div>

<h3 id="会话">会话</h3>

<p>接下来的代码就是创建一个Session对象，调用它的run方法。只有这样，才能进行真正的运算。如下所示：</p>

<pre><code class="language-Python">with tf.Session() as sess:
    print sess.run([c1,c2])

# 输出： [array([0.3, 2. ], dtype=float32), array([1.3, 1.2], dtype=float32)]

</code></pre>

<p>这时，我们就能得到了想象中的两个值。</p>

<p>我们可以通过联合多个张量(Tensors)，构建更为复杂的运算。例如，我们可以进行点张量的相加，如下所示：</p>

<pre><code class="language-Python">import tensorflow as tf
import numpy as np

c1 = tf.constant([0.3,2.0],tf.float32)
c2 = tf.constant([1.3,1.2],tf.float32)

op_add = tf.add(c1,c2)

with tf.Session() as sess:
    print "op_add rst:{}".format(sess.run(op_add))

# 输出：op_add rst:[1.5999999 3.2      ]
</code></pre>

<h3 id="占位符placeholder">占位符(Placeholder)</h3>

<p>有些时候，我们不会直接使用常量进行计算，而需要事先创建一个量去表示运算。这时，在TensorFlow中可以采用placeholders的方法。它的本质就是一个占位符，先用placeholder表示进行运算的表达，程序后面在进行“喂值”(feed_dict)。如下所示：</p>

<pre><code class="language-Python"># placeholder
x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
op_add = tf.add(x,y)

</code></pre>

<p>由上面操作可见，x和y都没有固定的值，它们创建的目的只是为了表示相加这种运算。</p>

<p>如果程序后面需要用到这种运算，我们可以使用feed_dict进行喂值。</p>

<pre><code class="language-Python">cc =sess.run(op_add,{x:[1.0,2.0,3,4],y:[2,3,4,1]})
print cc
# 输出 [3. 5. 7. 5.]
</code></pre>

<p>显然，不同的“喂值”，得出的结果可能也不同。</p>

<p>在基础上，我们可以进行更复杂的操作。我们可以对op_add进行相乘操作。如下所示:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">c1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">c2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.3</span><span class="p">,</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">op_add</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span><span class="n">c2</span><span class="p">)</span>
<span class="n">triple</span><span class="o">=</span><span class="n">op_add</span><span class="o">*</span><span class="mi">3</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="k">print</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">c1</span><span class="p">,</span><span class="n">c2</span><span class="p">])</span>
    <span class="k">print</span> <span class="s">"triple rst:{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">triple</span><span class="p">))</span>

<span class="c"># 输出：triple rst:4.7999997139</span>
</code></pre></div></div>

<p>当然，要显示最终结果，需要进行回话和喂值。</p>

<h3 id="变量variable">变量(Variable)</h3>

<p>机器学习中，我们构建模型，TensorFlow中模型有输入有输出，训练中参数会不断更新，这时，我们需要创建变量(Variable)。TensorFlow中，Variable可以将训练参数添加到图中，声明变量时，一般需要声明变量的类型（如：tf.float32）和赋初值。如下所示：</p>

<pre><code class="language-Python">x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
op_add = tf.add(x,y)
op_tri = op_add*3
</code></pre>

<p>这样，我们就成功地创建了变量。从上面例子，我们可以看出placeholder和Variable的区别。当该量是通过训练更新的，我们可以通过Variable创建，当该量是作为模型的输入，我们可以通过placeholder创建。</p>

<p>当然，变量的启动不仅需要开启会话，还要在会话中进行初始化。</p>

<pre><code class="language-Python">with tf.Session() as sess:
    init = tf.global_variables_initializer()
    sess.run(init)
</code></pre>

<p>这样，变量才真正地被初始化。</p>

<p>初始化成功之后，我们给模型喂值，</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="mf">1.1</span><span class="p">],</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="o">-</span><span class="mf">2.1</span><span class="p">],</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>
<span class="n">linear_mode</span><span class="o">=</span><span class="n">W</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"real vale: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">linear_mode</span><span class="p">,{</span><span class="n">x</span><span class="p">:</span><span class="n">x_train</span><span class="p">})))</span>

</code></pre></div></div>

<p>其实，这里我们构建了一个很简单的线性模型。我们希望该模型能够进行训练优化，然而，我们没有设定训练集的期望值。因此，我们自定义期望值。</p>

<p>优化训练，我们采用最小二乘法。</p>

<p>代价函数为：cost(x) = sum(linear_model(x) - y)/number of trainset</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">square_details</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">linear_mode</span><span class="o">-</span> <span class="n">y</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">square_details</span><span class="p">)</span>
</code></pre></div></div>

<p>得到了损失函数，我们就可以对模型的准确性进行评估了。</p>

<h3 id="tensorboard">tensorboard</h3>

<p>Tensorboard可以记录与展示以下数据形式：</p>
<ul>
  <li>标量Scalars</li>
  <li>图片Images</li>
  <li>音频Audio</li>
  <li>计算图Graph</li>
  <li>数据分布Distribution</li>
  <li>直方图Histograms</li>
  <li>嵌入向量Embeddings</li>
</ul>

<p>Tensorboard的可视化过程</p>

<ul>
  <li>
    <p>首先肯定是先建立一个graph,你想从这个graph中获取某些数据的信息</p>
  </li>
  <li>
    <p>确定要在graph中的哪些节点放置summary operations以记录信息</p>
  </li>
</ul>

<p><b>使用tf.summary.scalar记录标量</b></p>

<p>常量则可使用Tensorflow.scalar_summary()方法：</p>

<p>tf.scalar_summary(‘loss’,loss) #命名和赋值</p>

<p><img src="/img/in-post/post-tf/tf39.jpeg" alt="" /></p>

<p><b>使用tf.summary.histogram记录数据的直方图</b></p>

<p><img src="/img/in-post/post-tf/tf37.jpeg" alt="" /></p>

<ul>
  <li>
    <p>operations并不会去真的执行计算，除非你告诉他们需要去run,或者它被其他的需要run的operation所依赖。而我们上一步创建的这些summary operations其实并不被其他节点依赖，因此，我们需要特地去运行所有的summary节点。但是呢，一份程序下来可能有超多这样的summary 节点，要手动一个一个去启动自然是及其繁琐的，因此我们可以使用tf.summary.merge_all去将所有summary节点合并成一个节点，只要运行这个节点，就能产生所有我们之前设置的summary data。</p>
  </li>
  <li>
    <p>使用tf.summary.FileWriter将运行后输出的数据都保存到本地磁盘中</p>
  </li>
  <li>
    <p>运行整个程序，并在命令行输入运行tensorboard的指令，之后打开web端可查看可视化的结果</p>
  </li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c">#合并到Summary中  </span>
<span class="n">merged</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">merge_all_summaries</span><span class="p">()</span>  
<span class="c">#选定可视化存储目录  </span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">SummaryWriter</span><span class="p">(</span><span class="s">"/目录"</span><span class="p">,</span><span class="n">sess</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>  

<span class="c">#merged也是需要run的  </span>
<span class="n">result</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">merged</span><span class="p">)</span>
<span class="n">writer</span><span class="o">.</span><span class="n">add_summary</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">step</span><span class="p">)</span>  
</code></pre></div></div>
<p><img src="/img/in-post/post-tf/tf38.jpeg" alt="" /></p>

<p>如果6006端口被占用，会报一下错误：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ERROR:tensorflow:Tried to connect to port 6006, but address is in use.
Tried to connect to port 6006, but address is in use.
</code></pre></div></div>

<p>解决可以使用 –port 指定端口：</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensorboard <span class="nt">--host</span><span class="o">=</span>10.10.101.2 <span class="nt">--port</span><span class="o">=</span>6099 <span class="nt">--logdir</span><span class="o">=</span><span class="s2">"my_graph"</span>
</code></pre></div></div>


                <hr style="visibility: hidden;">

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2018/03/07/pil/" data-toggle="tooltip" data-placement="top" title="Python Pillow做一个强大的图形工具">
                        Previous<br>
                        <span>Python Pillow做一个强大的图形工具</span>
                        </a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2018/03/09/tensorflow/" data-toggle="tooltip" data-placement="top" title="TensorFlow-神经网络">
                        Next<br>
                        <span>TensorFlow-神经网络</span>
                        </a>
                    </li>
                    
                </ul>


                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

                
            </div>

    <!-- Side Catalog Container -->
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <hr class="hidden-sm hidden-xs">
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
        				          
                				       <a href="/tags/#OSX" title="OSX" rel="4">OSX</a>
        				          
                				       <a href="/tags/#前端开发" title="前端开发" rel="15">前端开发</a>
        				          
                				       <a href="/tags/#JavaScript" title="JavaScript" rel="8">JavaScript</a>
        				          
                				       <a href="/tags/#Lua" title="Lua" rel="1">Lua</a>
        				          
                				       <a href="/tags/#Unity" title="Unity" rel="16">Unity</a>
        				          
                				       <a href="/tags/#手机游戏" title="手机游戏" rel="7">手机游戏</a>
        				          
                				       <a href="/tags/#Jenkins" title="Jenkins" rel="1">Jenkins</a>
        				          
                				       <a href="/tags/#C++" title="C++" rel="3">C++</a>
        				          
                				       <a href="/tags/#Tensorflow" title="Tensorflow" rel="6">Tensorflow</a>
        				          
                				       <a href="/tags/#人工智能" title="人工智能" rel="7">人工智能</a>
        				          
                				       <a href="/tags/#Python" title="Python" rel="8">Python</a>
        				          
                				       <a href="/tags/#工具" title="工具" rel="5">工具</a>
        				          
                				       <a href="/tags/#强化学习" title="强化学习" rel="4">强化学习</a>
        				          
        			      </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">
                    
                        <li><a href="http://www.gamelook.com.cn">GameLook</a></li>
                    
                        <li><a href="http://forum.china.unity3d.com/forum.php">游戏人社区</a></li>
                    
                        <li><a href="https://unity3d.com">Unity3D</a></li>
                    
                        <li><a href="https://www.uwa4d.com">UWA-简单优化</a></li>
                    
                        <li><a href="https://blog.openai.com/openai-baselines-ppo/">OpenAI新的策略优化算法PPO</a></li>
                    
                        <li><a href="https://www.zhihu.com/people/morvan/activities">知乎-莫烦 Python</a></li>
                    
                        <li><a href="http://blog.sina.com.cn/s/blog_471132920101d5kh.html">风宇冲的 Blog</a></li>
                    
                        <li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist_beginners.html">极客学院-Tensotflow</a></li>
                    
                        <li><a href="https://unity3d.com/cn/solutions/mobile-ar">Unity for Mobile AR</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>






<!-- disqus 公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "Huailiang";
    var disqus_identifier = "/2018/03/08/tensor";
    var disqus_url = "http://localhost:4000/2018/03/08/tensor/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus 公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'always',
          placement: 'right',
          icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                     

                    <!-- add Weibo, Zhihu by Hux, add target = "_blank" to <a> by Hux -->
                    
                    <li>
                        <a target="_blank" href="https://www.zhihu.com/">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa  fa-stack-1x fa-inverse">知</i>
                            </span>
                        </a>
                    </li>
                     
                    <li>
                        <a target="_blank" href="https://weibo.com/u/6212299692">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                     
                    <li>
                        <a target="_blank" href="https://www.facebook.com/profile.php?id=100004290725320">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                     
                    <li>
                        <a target="_blank" href="https://github.com/huailiang">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                     
                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Huailiang Blog 2018
                    <br> Power by
                    <a href="http://huailiang.github.io">Huailiang</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="100px" height="20px" src="https://ghbtns.com/github-btn.html?user=huailiang&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src=" /js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src=" /js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src=" /js/hux-blog.min.js "></script>

<!-- Service Worker -->

<script src=" /js/snackbar.js "></script>
<script src=" /js/sw-registration.js "></script> 

<!-- async load function -->
<script>
    function async(u, c) {
        var d = document, t = 'script',
            o = d.createElement(t),
            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
        s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if ($('#tag_cloud').length !== 0) {
        async('/js/jquery.tagcloud.js', function () {
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: { start: '#bbbbee', end: '#0085a1' },
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function () {
        var $nav = document.querySelector("nav");
        if ($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->

<script>
    // dynamic User by Hux
    var _gaId = 'UA-49627206-1';
    var _gaDomain = 'github.io';

    // Originial
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script> 


<!-- Baidu Tongji -->



<!-- Side Catalog -->



<!-- Multi-Lingual -->



<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
