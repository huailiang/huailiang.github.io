<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>移动端AI算计加速</title>
  <meta name="description"
    content="  目前支持移动端（手机端）的框架， 主流的实现架构诸如苹果公司的CoreML, 华为麒麟芯片的支持AI加速的NPU和在上面运行框架HiAI，阿里的优化框架MNN， 还有腾讯公司的NCNN。 过去我们使用的AI运算大都是PC端搭建的GPU集群，往往运行着后端（云侧）， 而在端侧往往只是负责拿到云侧数据进行表现。 ...">

  <link rel="shortcut icon" href="/img/favicon.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://huailiang.github.io/blog/2021/mnn/">

</head>

<body>
  <main>
    <header class="site-header">
  <div class="container">
    <h1><a href="/">Hom<span>e</span></a></h1>

    <button type="button" class="sliding-panel-button">
      <span></span>
      <span></span>
      <span></span>
    </button>

    <nav class="navbar sliding-panel-content">
      <ul>
        
        <li><a href="/about" title="About">About</a>
        </li>
        
        <li><a href="/blog" title="Blog">Blog</a>
        </li>
        
       <li><a href="/category/" target="_blank"><i class="icon icon-feed"></i></a></li>
      </ul>
    </nav>
  </div>
</header>

<div class="sliding-panel-fade-screen"></div>
    <script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });

</script>
    <div class="container">
      <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">移动端AI算计加速</h1>
      <p class="post-meta">May 26, 2021 •
        huailiang</p>
    </header>

    <div class="post-content">
      <blockquote>
  <p>目前支持移动端（手机端）的框架， 主流的实现架构诸如苹果公司的CoreML, 华为麒麟芯片的支持AI加速的NPU和在上面运行框架HiAI，阿里的优化框架MNN， 还有腾讯公司的NCNN。 过去我们使用的AI运算大都是PC端搭建的GPU集群，往往运行着后端（云侧）， 而在端侧往往只是负责拿到云侧数据进行表现。 随着手机端性能的提升， 其巨大的算力给AI拓扑带来了无限的可能。</p>
</blockquote>

<h4 id="一概述">一、概述</h4>

<p>目前AI的主流实现都是基于反向传播算法的神经网络，前向传播进行推理， 反向传播计算梯度来更新参数。 主机上运行的主流框架多是谷歌公司的Tensorflow (Lite), Facebook公司开发的Pytorch 以及 Caffe, 然后他们在设计这些框架的时候， 往往更多的考虑的是算法的覆盖， 而不是性能。 我们常常看到这些复杂的网络结构大多跑在Nvidia的高端显卡上， 训练时间往往短则数日， 长则数周以致多大一两个月， 比如说Nvidia实现的StyleGAN。 由于这些特性， 特别不适应手机端的芯片。</p>

<p><img src="/img/post-ml/mnn.jpeg" alt="" /></p>

<p>随着国内厂商不断的优化， 我们看到了华为推出的麒麟芯片（麒麟980之后带独立的NPU）使用<a href="https://developer.huawei.com/consumer/cn/hiai#Foundation">HiAI</a>框架来进行AI加速运算， <a href="https://github.com/alibaba/MNN">阿里的MNN</a>则依靠大量手写汇编实现核心运算，充分发挥 ARM CPU 的算力， 整合不同后端（backend: OpenCL、Vulkan、OpenGL, Metal)进行深度优化来适配不同的设备。这些国内的厂商都提供了各自的工具（比如说华为HiAI的转换工具OMG, 阿里的mmnconvert）来转换主机上运行的Tensorflow、Caffee模型为自己量化的模型。</p>

<p><img src="/img/post-ml/hiai.jpeg" alt="" /></p>

<h4 id="二环境">二、环境</h4>

<p>我们看到国内厂商推出的Demo都是基于原生的语言开发出来的应用， 目前对游戏引擎这种跨平台的应用支持的还不多，这里大多是由于移动端的算子支持还不够全面， 从而导致有些主机上的模型导致转换失败，这就限制了移动端从主机端迁移的速度， 不过类似经典的MobileNet_v2都是支持的。</p>

<h4 id="三-生成数据集">三. 生成数据集</h4>

<p>本文以手势识别为例，首先创建一个数据集， 提取mnist的数据， 然后通过PIL的接口导出图片和对应的标签。代码如下：</p>

<div class="language-py highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">tensorflow.examples.tutorials.mnist</span> <span class="kn">import</span> <span class="n">input_data</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s">'./MNIST_data'</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c"># hack for duplicated library on MacOS</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'KMP_DUPLICATE_LIB_OK'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'True'</span>

<span class="k">def</span> <span class="nf">save_raw</span><span class="p">():</span>
    <span class="n">save_dir</span> <span class="o">=</span> <span class="s">'raw/'</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span> <span class="ow">is</span> <span class="bp">False</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">)</span>

    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">"raw/flag.txt"</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">image_array</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">image_array</span> <span class="o">=</span> <span class="n">image_array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="n">save_dir</span> <span class="o">+</span> <span class="s">'mnist_</span><span class="si">%</span><span class="s">d.jpg'</span> <span class="o">%</span> <span class="n">i</span>
        <span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">((</span><span class="n">image_array</span> <span class="o">*</span> <span class="mi">255</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'uint8'</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s">'L'</span><span class="p">)</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s">'RGB'</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
        <span class="n">label_array</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
        <span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">label_array</span><span class="p">))</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">label_array</span><span class="p">)))</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="n">save_raw</span><span class="p">()</span>
</code></pre>
</div>

<h4 id="四-构建网络">四. 构建网络</h4>

<p>我们在python环境下， 使用cnn网络简单搭建一个手写图片预测的神经网络， 并保证预测的准确率在90%以上。 然后到处网络以pb格式存储。</p>

<p>如下代码， 为了使用mnn中NHWC格式的数据， 我们将数据集进入ai网络前需要把[None， 28x28, 1]的格式转成[None, 28, 28, 1]的输入格式</p>

<div class="language-py highlighter-rouge"><pre class="highlight"><code><span class="n">mnist</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">read_data_sets</span><span class="p">(</span><span class="s">'./MNIST_data'</span><span class="p">,</span> <span class="n">one_hot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c"># they has been normalized to range (0,1)</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">images</span><span class="p">[:</span><span class="mi">2000</span><span class="p">]</span>
<span class="n">test_x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">labels</span><span class="p">[:</span><span class="mi">2000</span><span class="p">]</span>

<span class="n">tf_x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="c"># image = tf.reshape(tf_x, [-1, 28, 28, 1])  # (batch, height, width, channel)</span>
<span class="n">tf_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>  <span class="c"># input y</span>

<span class="c"># CNN</span>
<span class="n">conv1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>  <span class="c"># shape (28, 28, 1)</span>
    <span class="n">inputs</span><span class="o">=</span><span class="n">tf_x</span><span class="p">,</span>
    <span class="n">filters</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span>
<span class="p">)</span>  <span class="c"># -&gt; (28, 28, 16)</span>
<span class="n">pool1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">max_pooling2d</span><span class="p">(</span>
    <span class="n">conv1</span><span class="p">,</span>
    <span class="n">pool_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>  <span class="c"># -&gt; (14, 14, 16)</span>
<span class="n">conv2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">pool1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'same'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>  <span class="c"># -&gt; (14, 14, 32)</span>
<span class="n">pool2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">max_pooling2d</span><span class="p">(</span><span class="n">conv2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c"># -&gt; (7, 7, 32)</span>
<span class="n">flat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pool2</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">32</span><span class="p">])</span>  <span class="c"># -&gt; (7*7*32, )</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">flat</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c"># output layer</span>
<span class="n">out_y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">"y_pred"</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">onehot_labels</span><span class="o">=</span><span class="n">tf_y</span><span class="p">,</span> <span class="n">logits</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>  <span class="c"># compute cost</span>
<span class="n">train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">LR</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="n">accuracy</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span>  <span class="c"># return (acc, update_op), and create 2 local variables</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf_y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">predictions</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">(),</span> <span class="n">tf</span><span class="o">.</span><span class="n">local_variables_initializer</span><span class="p">())</span>
<span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>  <span class="c"># initialize var in graph</span>

<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="n">b_x</span><span class="p">,</span> <span class="n">b_y</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">next_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="n">b_x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">b_x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">loss_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">train_op</span><span class="p">,</span> <span class="n">loss</span><span class="p">],</span> <span class="p">{</span><span class="n">tf_x</span><span class="p">:</span> <span class="n">b_x2</span><span class="p">,</span> <span class="n">tf_y</span><span class="p">:</span> <span class="n">b_y</span><span class="p">})</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">accuracy_</span><span class="p">,</span> <span class="n">flat_representation</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">flat</span><span class="p">],</span> <span class="p">{</span><span class="n">tf_x</span><span class="p">:</span> <span class="n">test_x2</span><span class="p">,</span> <span class="n">tf_y</span><span class="p">:</span> <span class="n">test_y</span><span class="p">})</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Step:'</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="s">'| train loss: </span><span class="si">%.4</span><span class="s">f'</span> <span class="o">%</span> <span class="n">loss_</span><span class="p">,</span> <span class="s">'| test accuracy: </span><span class="si">%.2</span><span class="s">f'</span> <span class="o">%</span> <span class="n">accuracy_</span><span class="p">)</span>

<span class="n">test_output</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">out_y</span><span class="p">,</span> <span class="p">{</span><span class="n">tf_x</span><span class="p">:</span> <span class="n">test_x2</span><span class="p">[:</span><span class="mi">10</span><span class="p">]})</span>
<span class="c"># pred_y = np.argmax(test_output, 1)</span>
<span class="k">print</span><span class="p">(</span><span class="n">test_output</span><span class="p">,</span> <span class="s">'prediction number'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">test_y</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="s">'real number'</span><span class="p">)</span>

<span class="n">output_graph_def</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">graph_util</span><span class="o">.</span><span class="n">convert_variables_to_constants</span><span class="p">(</span>  <span class="c"># 模型持久化，将变量值固定</span>
    <span class="n">sess</span><span class="p">,</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">graph_def</span><span class="p">,</span>
    <span class="p">[</span><span class="s">'y_pred'</span><span class="p">]</span>  <span class="c"># 如果有多个输出节点，以逗号隔开</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">gfile</span><span class="o">.</span><span class="n">GFile</span><span class="p">(</span><span class="s">"model/mnist.pb"</span><span class="p">,</span> <span class="s">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>  <span class="c"># 保存模型</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">output_graph_def</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
</code></pre>
</div>

<p>然后将训练好的pb文件转换为mnn能够识别的格式</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code>./MNNConvert -f TF --modelFile mnist.pb --MNNModel mnist.mnn --bizCode biz
</code></pre>
</div>

<p>MNNConvert具体的转换参数参见<a href="https://www.yuque.com/mnn/cn/model_convert">官方</a>.</p>

<h4 id="五-unity侧wrap接口">五. Unity侧Wrap接口</h4>

<p>官方Demo提供了一个java表面的wrap接口， 我们这里为了在Unity引擎的实现同样的效果， 封装了一套c#的Wrap接口。 具体参见 <a href="https://github.com/huailiang/mnn_u3d/blob/master/Assets/Scripts/MNN/MnnCV.cs">MnnApi.cs</a></p>

<p>对应的在c++侧实现一套和c#对接的接口， mnistnative.cpp 并通过cmakelist， 将c++侧的接口编译成so， 导入到Unity的Plugins目录</p>

<div class="language-c++ highlighter-rouge"><pre class="highlight"><code><span class="cp">#include &lt;android/bitmap.h&gt;
#include &lt;jni.h&gt;
#include &lt;string&gt;
#include &lt;MNN/ImageProcess.hpp&gt;
#include &lt;MNN/Interpreter.hpp&gt;
#include &lt;MNN/Tensor.hpp&gt;
#include &lt;memory&gt;
#include &lt;vector&gt;
#include &lt;android/log.h&gt;
</span>
<span class="cp">#define LOG_TAG "JNI.out"
#define LOGI(...) __android_log_print(ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)
</span>
<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">void</span> <span class="o">*</span><span class="n">JNICALL</span> <span class="n">nativeCreateNetFromFile</span><span class="p">(</span><span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">modelName</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">interpreter</span> <span class="o">=</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span><span class="o">::</span><span class="n">createFromFile</span><span class="p">(</span><span class="n">modelName</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">interpreter</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="n">JNICALL</span> <span class="n">nativeReleaseNet</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">nullptr</span> <span class="o">==</span> <span class="n">netPtr</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">delete</span> <span class="p">((</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="n">JNICALL</span> <span class="n">nativeCreateSess</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">forwardType</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numThread</span><span class="p">,</span>
                                                  <span class="kt">int</span> <span class="o">&amp;</span><span class="n">saveSize</span><span class="p">,</span> <span class="kt">int</span> <span class="o">&amp;</span><span class="n">outputSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"nativeCreateSess forward, %d, numthread: %d "</span><span class="p">,</span> <span class="n">forwardType</span><span class="p">,</span> <span class="n">numThread</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">forwardType</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">void</span> <span class="o">*</span><span class="n">JNICALL</span> <span class="n">nativeCreateSession</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">forwardType</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numThread</span><span class="p">,</span>
                                                       <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">*</span><span class="n">jsaveTensors</span><span class="p">,</span> <span class="kt">int</span> <span class="o">&amp;</span><span class="n">saveSize</span><span class="p">,</span>
                                                       <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">*</span><span class="n">joutputTensors</span><span class="p">,</span> <span class="kt">int</span> <span class="o">&amp;</span><span class="n">outputSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">MNN</span><span class="o">::</span><span class="n">ScheduleConfig</span> <span class="n">config</span><span class="p">;</span>
    <span class="n">config</span><span class="p">.</span><span class="n">type</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNNForwardType</span><span class="p">)</span> <span class="n">forwardType</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">numThread</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="n">config</span><span class="p">.</span><span class="n">numThread</span> <span class="o">=</span> <span class="n">numThread</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">jsaveTensors</span> <span class="o">!=</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">saveSize</span><span class="p">;</span><span class="c1">// env-&gt;GetArrayLength(jsaveTensors);
</span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">saveNamesVector</span><span class="p">;</span>

        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">nameStr</span> <span class="o">=</span> <span class="n">jsaveTensors</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
            <span class="n">saveNamesVector</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">nameStr</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">config</span><span class="p">.</span><span class="n">saveTensors</span> <span class="o">=</span> <span class="n">saveNamesVector</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">joutputTensors</span> <span class="o">!=</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">size</span> <span class="o">=</span> <span class="n">outputSize</span><span class="p">;</span><span class="c1">// env-&gt;GetArrayLength(joutputTensors);
</span>        <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">saveNamesVector</span><span class="p">;</span>

        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">nameStr</span> <span class="o">=</span> <span class="n">joutputTensors</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
            <span class="n">saveNamesVector</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">nameStr</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="n">config</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">saveNamesVector</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="p">((</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">createSession</span><span class="p">(</span><span class="n">config</span><span class="p">);</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"nativeCreateSession  execute SUCC"</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">session</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">void</span> <span class="n">JNICALL</span> <span class="n">nativeReleaseSession</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">sessionPtr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">net</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Session</span> <span class="o">*</span><span class="p">)</span> <span class="n">sessionPtr</span><span class="p">;</span>
    <span class="n">net</span><span class="o">-&gt;</span><span class="n">releaseSession</span><span class="p">(</span><span class="n">session</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="n">JNICALL</span> <span class="n">nativeRunSession</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">sessionPtr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">net</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Session</span> <span class="o">*</span><span class="p">)</span> <span class="n">sessionPtr</span><span class="p">;</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"nativeRunSession:%d, %d"</span><span class="p">,(</span><span class="n">netPtr</span><span class="o">==</span> <span class="nb">nullptr</span><span class="p">),(</span><span class="n">sessionPtr</span><span class="o">==</span> <span class="nb">nullptr</span><span class="p">));</span>
    <span class="k">return</span> <span class="n">net</span><span class="o">-&gt;</span><span class="n">runSession</span><span class="p">(</span><span class="n">session</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="n">JNICALL</span>
<span class="n">nativeRunSessionWithCallback</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">sessionPtr</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">*</span><span class="n">nameArray</span><span class="p">,</span> <span class="kt">int</span> <span class="o">&amp;</span><span class="n">nameSize</span><span class="p">,</span>
                             <span class="kt">void</span> <span class="o">**</span><span class="n">tensoraddrs</span><span class="p">,</span> <span class="kt">int</span> <span class="o">&amp;</span><span class="n">tensorSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tensorSize</span> <span class="o">&lt;</span> <span class="n">nameSize</span><span class="p">)</span> <span class="n">MNN_ERROR</span><span class="p">(</span><span class="s">"tensor array not enough!"</span><span class="p">);</span>

    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span> <span class="n">nameVector</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nameSize</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">nameStr</span> <span class="o">=</span> <span class="n">nameArray</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
        <span class="n">nameVector</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">nameStr</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MNN</span><span class="o">::</span><span class="n">TensorCallBack</span> <span class="n">beforeCallBack</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">ntensors</span><span class="p">,</span>
                                             <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">&amp;</span><span class="n">opName</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
    <span class="p">};</span>

    <span class="n">MNN</span><span class="o">::</span><span class="n">TensorCallBack</span> <span class="n">AfterCallBack</span> <span class="o">=</span> <span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*&gt;</span> <span class="o">&amp;</span><span class="n">ntensors</span><span class="p">,</span>
                                            <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="o">&amp;</span><span class="n">opName</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nameVector</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">nameVector</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">==</span> <span class="n">opName</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">auto</span> <span class="n">ntensor</span> <span class="o">=</span> <span class="n">ntensors</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>

                <span class="k">auto</span> <span class="n">outputTensorUser</span> <span class="o">=</span> <span class="k">new</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">ntensor</span><span class="p">,</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="o">::</span><span class="n">TENSORFLOW</span><span class="p">);</span>
                <span class="n">ntensor</span><span class="o">-&gt;</span><span class="n">copyToHostTensor</span><span class="p">(</span><span class="n">outputTensorUser</span><span class="p">);</span>
                <span class="n">tensoraddrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputTensorUser</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
    <span class="p">};</span>

    <span class="k">auto</span> <span class="n">net</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Session</span> <span class="o">*</span><span class="p">)</span> <span class="n">sessionPtr</span><span class="p">;</span>

    <span class="n">net</span><span class="o">-&gt;</span><span class="n">runSessionWithCallBack</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">beforeCallBack</span><span class="p">,</span> <span class="n">AfterCallBack</span><span class="p">,</span> <span class="nb">true</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="n">JNICALL</span> <span class="n">nativeReshapeSession</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">sessionPtr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">net</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Session</span> <span class="o">*</span><span class="p">)</span> <span class="n">sessionPtr</span><span class="p">;</span>
    <span class="n">net</span><span class="o">-&gt;</span><span class="n">resizeSession</span><span class="p">(</span><span class="n">session</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">void</span> <span class="o">*</span><span class="n">JNICALL</span>
<span class="n">nativeGetSessionInput</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">sessionPtr</span><span class="p">,</span> <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">name</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">net</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Session</span> <span class="o">*</span><span class="p">)</span> <span class="n">sessionPtr</span><span class="p">;</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"name %s, %d, %ld"</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">),</span> <span class="p">(</span><span class="kt">long</span><span class="p">)</span> <span class="n">sessionPtr</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">nullptr</span> <span class="o">==</span> <span class="n">name</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">net</span><span class="o">-&gt;</span><span class="n">getSessionInput</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">net</span><span class="o">-&gt;</span><span class="n">getSessionInput</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">name</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">void</span> <span class="o">*</span>
<span class="n">JNICALL</span> <span class="n">nativeGetSessionOutput</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">sessionPtr</span><span class="p">,</span> <span class="k">const</span> <span class="kt">char</span> <span class="o">*</span><span class="n">name</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">net</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">session</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Session</span> <span class="o">*</span><span class="p">)</span> <span class="n">sessionPtr</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">nullptr</span> <span class="o">==</span> <span class="n">name</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">net</span><span class="o">-&gt;</span><span class="n">getSessionOutput</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="nb">nullptr</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="n">net</span><span class="o">-&gt;</span><span class="n">getSessionOutput</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">name</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">void</span> <span class="n">JNICALL</span>
<span class="n">nativeReshapeTensor</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">tensorPtr</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">dims</span><span class="p">,</span> <span class="kt">int</span> <span class="n">dimSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"nativeReshapeTensor:  %d, %d, %d"</span><span class="p">,</span>  <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"dimsize: %d"</span><span class="p">,</span> <span class="n">dimSize</span><span class="p">);</span>
    <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">dimVector</span><span class="p">(</span><span class="n">dimSize</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dimSize</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">dimVector</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">dims</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="k">auto</span> <span class="n">net</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Interpreter</span> <span class="o">*</span><span class="p">)</span> <span class="n">netPtr</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="p">)</span> <span class="n">tensorPtr</span><span class="p">;</span>
    <span class="n">net</span><span class="o">-&gt;</span><span class="n">resizeTensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dimVector</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">void</span> <span class="n">JNICALL</span>
<span class="n">nativeSetInputIntData</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">tensorPtr</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="o">*</span><span class="n">data</span><span class="p">,</span> <span class="kt">int</span> <span class="n">dataSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="p">)</span> <span class="n">tensorPtr</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dataSize</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">void</span> <span class="n">JNICALL</span>
<span class="n">nativeSetInputFloatData</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">netPtr</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">tensorPtr</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">data</span><span class="p">,</span> <span class="kt">int</span> <span class="n">dataSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="p">)</span> <span class="n">tensorPtr</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dataSize</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="o">*</span><span class="n">JNICALL</span> <span class="n">nativeTensorGetDimensions</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">tensorPtr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="p">)</span> <span class="n">tensorPtr</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">dimensions</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">buffer</span><span class="p">().</span><span class="n">dimensions</span><span class="p">;</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">destDims</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">int</span><span class="p">[</span><span class="n">dimensions</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dimensions</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">destDims</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">length</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
        <span class="n">LOGI</span><span class="p">(</span><span class="s">"dim %d is %d"</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">destDims</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">destDims</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="n">JNICALL</span>
<span class="n">nativeTensorGetUINT8Data</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">tensorPtr</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span><span class="n">destPtr</span><span class="p">,</span> <span class="kt">int</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="p">)</span> <span class="n">tensorPtr</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">nullptr</span> <span class="o">==</span> <span class="n">destPtr</span><span class="p">)</span> <span class="k">return</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">hostTensor</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">()</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// GPU buffer
</span>        <span class="n">hostTensor</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">getDimensionType</span><span class="p">(),</span> <span class="nb">true</span><span class="p">));</span>
        <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">copyToHostTensor</span><span class="p">(</span><span class="n">hostTensor</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">hostTensor</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
    <span class="p">}</span>

    <span class="k">auto</span> <span class="n">size</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">length</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">MNN_ERROR</span><span class="p">(</span><span class="s">"Can't copy buffer, length no enough"</span><span class="p">);</span>
        <span class="k">return</span> <span class="n">JNI_FALSE</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="o">::</span><span class="n">memcpy</span><span class="p">(</span><span class="n">destPtr</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">uint8_t</span><span class="p">));</span>
<span class="c1">//    env-&gt;ReleaseByteArrayElements(jdest, destPtr, 0);
</span>    <span class="k">return</span> <span class="n">JNI_TRUE</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="n">JNICALL</span> <span class="n">nativeTensorGetIntData</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">tensorPtr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="p">)</span> <span class="n">tensorPtr</span><span class="p">;</span>

    <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">hostTensor</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">()</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// GPU buffer
</span>        <span class="n">hostTensor</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="k">new</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">getDimensionType</span><span class="p">(),</span> <span class="nb">true</span><span class="p">));</span>
        <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">copyToHostTensor</span><span class="p">(</span><span class="n">hostTensor</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">hostTensor</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">t</span><span class="o">=</span>  <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">();</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"RESULT: %d"</span><span class="p">,</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
    <span class="k">return</span> <span class="n">t</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">int</span> <span class="n">JNICALL</span> <span class="n">nativeTensorGetData</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">tensorPtr</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">dest</span><span class="p">,</span> <span class="kt">int</span> <span class="n">length</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">tensorPtr</span><span class="p">);</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"nativeTensorGetData %d"</span><span class="p">,(</span><span class="n">dest</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">));</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">nullptr</span> <span class="o">==</span> <span class="n">dest</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">hostTensor</span><span class="p">(</span>
                <span class="k">new</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">getDimensionType</span><span class="p">(),</span> <span class="nb">false</span><span class="p">));</span>
        <span class="k">return</span> <span class="n">hostTensor</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="n">hostTensor</span><span class="p">(</span>
            <span class="k">new</span> <span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">getDimensionType</span><span class="p">(),</span> <span class="nb">true</span><span class="p">));</span>
    <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">copyToHostTensor</span><span class="p">(</span><span class="n">hostTensor</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">hostTensor</span><span class="p">.</span><span class="n">get</span><span class="p">();</span>

    <span class="k">auto</span> <span class="n">size</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">elementSize</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">length</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">MNN_ERROR</span><span class="p">(</span><span class="s">"Can't copy buffer, length no enough"</span><span class="p">);</span>
        <span class="k">return</span> <span class="n">JNI_FALSE</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="o">::</span><span class="n">memcpy</span><span class="p">(</span><span class="n">dest</span><span class="p">,</span> <span class="n">tensor</span><span class="o">-&gt;</span><span class="n">host</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(),</span> <span class="n">size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
    <span class="n">LOGI</span><span class="p">(</span><span class="s">"result0 %f"</span><span class="p">,</span> <span class="n">dest</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
    <span class="k">return</span> <span class="n">JNI_TRUE</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">extern</span> <span class="s">"C"</span> <span class="n">JNIEXPORT</span> <span class="kt">bool</span> <span class="n">JNICALL</span>
<span class="n">nativeConvertBufferToTensor</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">bufferData</span><span class="p">,</span> <span class="kt">int</span> <span class="n">width</span><span class="p">,</span> <span class="kt">int</span> <span class="n">height</span><span class="p">,</span><span class="kt">int</span> <span class="n">format</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">tensorPtr</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">bufferData</span> <span class="o">==</span> <span class="nb">nullptr</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">MNN_ERROR</span><span class="p">(</span><span class="s">"Error Buffer Null!</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
        <span class="k">return</span> <span class="n">JNI_FALSE</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">MNN</span><span class="o">::</span><span class="n">CV</span><span class="o">::</span><span class="n">ImageProcess</span><span class="o">::</span><span class="n">Config</span> <span class="n">config</span><span class="p">;</span>
    <span class="n">config</span><span class="p">.</span><span class="n">destFormat</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">CV</span><span class="o">::</span><span class="n">ImageFormat</span><span class="p">)</span> <span class="n">format</span><span class="p">;</span>
    <span class="n">config</span><span class="p">.</span><span class="n">sourceFormat</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">CV</span><span class="o">::</span><span class="n">ImageFormat</span><span class="p">)</span> <span class="n">format</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">MNN</span><span class="o">::</span><span class="n">CV</span><span class="o">::</span><span class="n">ImageProcess</span><span class="o">&gt;</span> <span class="n">process</span><span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">CV</span><span class="o">::</span><span class="n">ImageProcess</span><span class="o">::</span><span class="n">create</span><span class="p">(</span><span class="n">config</span><span class="p">));</span>
    <span class="k">auto</span> <span class="n">tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">MNN</span><span class="o">::</span><span class="n">Tensor</span> <span class="o">*</span><span class="p">)</span> <span class="n">tensorPtr</span><span class="p">;</span>
    <span class="n">process</span><span class="o">-&gt;</span><span class="n">convert</span><span class="p">((</span><span class="k">const</span> <span class="kt">unsigned</span> <span class="kt">char</span> <span class="o">*</span><span class="p">)</span> <span class="n">bufferData</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tensor</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">JNI_TRUE</span><span class="p">;</span>
<span class="p">}</span>
</code></pre>
</div>

<p>将上述代码编译成libmnistcore.so, 并将对应的依赖的so同样copy到Plugins目录</p>

<p><img src="/img/post-ml/mnn2.jpg" alt="" /></p>

<p>导出apk的时候， 需要将打包方式选择IL2CPP, 然后字在手机上运行得到的结果如下图， 不断的点击Predict来切换数据集的图片， ai运行和记录的标签对比：</p>

<p><img src="/img/post-ml/mnn.jpg" alt="" /></p>

<p>相应的代码已经上传到 <a href="https://github.com/huailiang/mnn_u3d">Github</a></p>

<h4 id="六-华为npu的支持">六. 华为NPU的支持</h4>

<p>在编译libMNN的时候放开宏 -DMNN_NPU:BOOL=true ， 然后运行时选择backend方式MNN_FORWARD_USER_0，同时拷贝hiai的依赖库拷贝到Plugins目录下， 就可以麒麟芯片NPU上运行了。</p>

<p>具体的操作步骤参见<a href="https://www.yuque.com/mnn/cn/xfs77m">官方</a></p>


    </div>
  </div>

  <style>
  .post-nav {
    overflow: hidden;
    /* margin-top: 60px; */
    padding: 12px;
    white-space: nowrap;
    /* border-top: 1px solid #eee; */
  }

  .post-nav-item {
    display: inline-block;
    width: 50%;
    white-space: normal;
  }

  .post-nav-item a {
    position: relative;
    display: inline-block;
    line-height: 25px;
    font-size: 14px;
    color: #555;
    border-bottom: none;
  }

  .post-nav-item a:hover {
    color: #222;
    font-weight: bold;
    border-bottom: none;
  }

  .post-nav-item a:active {
    top: 2px;
  }

  .post-nav-item a:before,
  .post-nav-item a:after {
    display: inline-block;
    width: 16px;
    height: 25px;
    vertical-align: top;
    opacity: 0.4;
    background-size: 16px;
  }

  .post-nav-none a:none {
    content: ' ';
    background-size: 8px;
  }

  .post-nav-none a:hover:none {
    opacity: 1;
  }

  .post-nav-prev a:before {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjE4LjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLWxlZnQiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIxOC41MDAwMDAsIDkwLjAwMDAwMCkiPjxwYXRoIGQ9Ik03LjQsMS40IEw2LDAgTC04Ljg4MTc4NDJlLTE2LDYgTDYsMTIgTDcuNCwxMC42IEwyLjgsNiBMNy40LDEuNCBaIiBpZD0iU2hhcGUiLz48L2c+PC9nPjwvZz48L3N2Zz4=") no-repeat 0 50%;
    background-size: 8px;
  }

  .post-nav-prev a:hover:before {
    opacity: 1;
  }

  .post-nav-next {
    text-align: right;
  }

  .post-nav-next a:after {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjYwLjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLXJpZ2h0IiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgyNjAuNTAwMDAwLCA5MC4wMDAwMDApIj48cGF0aCBkPSJNMSwwIEwtMC40LDEuNCBMNC4yLDYgTC0wLjQsMTAuNiBMMSwxMiBMNyw2IEwxLDAgWiIgaWQ9IlNoYXBlIi8+PC9nPjwvZz48L2c+PC9zdmc+") no-repeat 100% 50%;
    background-size: 8px;
  }

  .post-nav-next a:hover:after {
    opacity: 1;
  }
</style>



<div class="post-nav">

  
  <div class="post-nav-none post-nav-item">
    <a href=""> </a>
  </div>
  

  
  <div class="post-nav-next post-nav-item">
    
    <a href="/blog/2021/maven/"> AndroidStudio搭建本地Maven仓库</a>
  </div>
  

</div>

</article>
    </div>

    <footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="https://twitter.com/penghuailiang" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="https://www.zhihu.com/people/huailiangpenguin" target="_blank"><i class="icon icon-zhihu"></i></a></li>
  <li><a href="https://www.facebook.com/profile.php?id=100004290725320" target="_blank"><i class="icon icon-facebook"></i></a></li>
  <li><a href="https://weibo.com/6212299692/profile?topnav=1&wvr=6" target="_blank"><i class="icon icon-sina"></i></a></li>
  <li><a href="https://www.linkedin.com/in/penghuailiang/" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="https://github.com/huailiang" target="_blank"><i class="icon icon-github"></i></a></li>
</ul>
    <p class="txt-medium-gray">
      <small>&copy;2021 All rights reserved. </small>
    </p>
  </div>
</footer>
    <a href="https://github.com/huailiang" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#337ab7; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script>
      $(document).ready(function () {
        $('.sliding-panel-button,.sliding-panel-fade-screen,.sliding-panel-close').on('click touchstart', function (e) {
          $('.sliding-panel-content,.sliding-panel-fade-screen').toggleClass('is-visible');
          e.preventDefault();
        });
      });
    </script>
  </main>
</body>

</html>