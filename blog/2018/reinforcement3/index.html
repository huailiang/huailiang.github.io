<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>强化学习-游戏AI Trainning (三)</title>
  <meta name="description"
    content="  前两节我们学会了利用q_learning来做游戏强化学习，并能提取出来再外部训练，下面一节我们继续深入学习使用DQN神经网络来训练我们的模型。">

  <link rel="shortcut icon" href="/img/favicon.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://huailiang.github.io/blog/2018/reinforcement3/">
  <link rel="alternate" type="application/rss+xml" title="Huailiang Blog"
    href="https://huailiang.github.io/feed.xml" />

</head>

<body>
  <main>
    <header class="site-header">
  <div class="container">
    <h1><a href="/">Hom<span>e</span></a></h1>

    <button type="button" class="sliding-panel-button">
      <span></span>
      <span></span>
      <span></span>
    </button>

    <nav class="navbar sliding-panel-content">
      <ul>
        
        <li><a href="/about" title="About">About</a>
        </li>
        
        <li><a href="/blog" title="Blog">Blog</a>
        </li>
        
        <!-- <li><a href="/feed.xml" target="_blank"><i class="icon icon-feed"></i></a></li> -->
        <li><a href="/category/" target="_blank"><i class="icon icon-feed"></i></a></li>
      </ul>
    </nav>
  </div>
</header>

<div class="sliding-panel-fade-screen"></div>
    <script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });

</script>
    <div class="container">
      <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">强化学习-游戏AI Trainning (三)</h1>
      <p class="post-meta">Mar 23, 2018 •
        Huailiang</p>
    </header>

    <div class="post-content">
      <blockquote>
  <p>前两节我们学会了利用q_learning来做游戏强化学习，并能提取出来再外部训练，下面一节我们继续深入学习使用DQN神经网络来训练我们的模型。</p>
</blockquote>

<p>完成本节内容，你需要本地有如下环境：</p>
<ul>
  <li>Python</li>
  <li>Pandas</li>
  <li>Numpy</li>
  <li>matplotlib（可选）</li>
  <li>Tensorflow</li>
  <li>Jupyter notebook(可选)</li>
</ul>

<h3 id="deepqnetwork">DeepQNetwork</h3>

<p>Deep Q Network 的简称叫 DQN, 是将 Q learning 的优势 和 Neural networks 结合了. 如果我们使用 tabular Q learning, 对于每一个 state, action 我们都需要存放在一张 q_table 的表中. 如果像显示生活中, 情况可就比那个迷宫的状况复杂多了, 我们有千千万万个 state, 如果将这千万个 state 的值都放在表中, 受限于我们计算机硬件, 这样从表中获取数据, 更新数据是没有效率的. 这就是 DQN 产生的原因了. 我们可以使用神经网络来 估算 这个 state 的值, 这样就不需要一张表了。</p>

<p>为了使用 Tensorflow 来实现 DQN, 比较推荐的方式是搭建两个神经网络, target_net 用于预测 q_target 值, 他不会及时更新参数. eval_net 用于预测 q_eval, 这个神经网络拥有最新的神经网络参数. 不过这两个神经网络结构是完全一样的, 只是里面的参数不一样。最终运行的Graph在tensorboard上可视化的结果如下图所示：</p>

<p><img src="/img/post-reinforcement/re12.jpg" alt="" /></p>

<h3 id="项目设置">项目设置</h3>

<p>本节所有的代码都上传到<a href="https://github.com/huailiang/bird">github</a>,需要联系的同学可以到github下载到本地练习。</p>

<p>训练的时候你需要将Unity导出mac或者windows的安装包，注意安装包需导出到Python目录之下。</p>

<p>Python的main.py需要做如下设置, from的模块选择的是dqn_environment，而不是environment：</p>

<p><img src="/img/post-reinforcement/re13.jpg" alt="" /></p>

<p>如果你安装了jupyter notebook的话，在terminal,cd到python所在的目录,，然后输入：</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">jupyter notebook</code></pre></figure>

<p>之后你可以在notebook里选择main.ipynb，进入主页：</p>

<p><img src="/img/post-reinforcement/re14.jpg" alt="" /></p>

<p>选择cell,Run就可以了。</p>

<p>eval_net用来训练模型，他的输入端是agent的状态值（state),输出的是得来的action对应的不同状态的数组，我们根据最大的q值选取相应的action</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">choose_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="c"># print observation</span>
    <span class="n">observation</span> <span class="o">=</span> <span class="n">observation</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
        <span class="c"># forward feed the observation and get q value for every actions</span>
        <span class="n">actions_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_eval</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">:</span> <span class="n">observation</span><span class="p">})</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">actions_value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">)</span></code></pre></figure>

<p>更新memory，在learn的过程中，我们每步我们都会在q_eval的memory储存信息，只是五步同步一次到q_target。实际运行的时候，在替换q_target之后，小鸟的智能明显提高了。</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">_to_learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">j</span><span class="p">):</span>
     <span class="n">state_</span> <span class="o">=</span> <span class="n">j</span><span class="p">[</span><span class="s">"state_"</span><span class="p">]</span>
     <span class="n">state</span>  <span class="o">=</span> <span class="n">j</span><span class="p">[</span><span class="s">"state"</span><span class="p">]</span>
     <span class="n">action</span> <span class="o">=</span> <span class="n">j</span><span class="p">[</span><span class="s">"action"</span><span class="p">]</span>
     <span class="n">rewd</span> <span class="o">=</span> <span class="n">j</span><span class="p">[</span><span class="s">"rewd"</span><span class="p">]</span>

     <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
         <span class="n">action</span> <span class="o">=</span> <span class="mi">0</span> <span class="c">#"pad"</span>
     <span class="k">else</span><span class="p">:</span>
         <span class="n">action</span> <span class="o">=</span> <span class="mi">1</span> <span class="c"># "stay"</span>

     <span class="n">state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">TransBrainState</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
     <span class="n">state_</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">TransBrainState</span><span class="p">(</span><span class="n">state_</span><span class="p">)</span>
     <span class="bp">self</span><span class="o">.</span><span class="n">RL</span><span class="o">.</span><span class="n">store_transition</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">rewd</span><span class="p">,</span><span class="n">state_</span><span class="p">)</span>
     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">&gt;</span> <span class="mi">20</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">RL</span><span class="o">.</span><span class="n">learn</span><span class="p">()</span>
     <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span></code></pre></figure>

<p>而在q_target神经网络的输入端，就是之前跟q_learning一样，包含如下信息：</p>

<ul>
  <li>state   当前agent的状态</li>
  <li>state_  下一步agent的状态</li>
  <li>reward  当前agent采取动作获的奖励</li>
  <li>action  当前agent采取的动作</li>
</ul>

<p>我们使用的神经网络输出得到的实际值和预估值做平方差再求均值来计算损失函数。</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># 说明：</span>
<span class="n">tf</span><span class="o">.</span><span class="n">squared_difference</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="c"># 功能：计算(x-y)(x-y)。</span>
<span class="c"># 输入：x为张量，可以为`half`,`float32`, `float64`类型。</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># 计算损失函数  </span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'loss'</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">squared_difference</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q_target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_eval</span><span class="p">))</span>

<span class="c"># 根据损失函数训练模型  </span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">'train'</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_train_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">RMSPropOptimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span></code></pre></figure>

<p>我们可以在若干步之后，打印出loss的变化，实现如下：</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plot_cost</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
     <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
     <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cost_his</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_his</span><span class="p">)</span>
     <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Cost'</span><span class="p">)</span>
     <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'training steps'</span><span class="p">)</span>
     <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<h3 id="结果">结果</h3>

<p>我们在tensorboard里观察数据的变化：</p>

<p>在terminal中cd 到对应的python目录，如果游戏已经运行了一段时间了，你可以看到本地多了一个logs的目录，这个本地生成对应的tensorboard的日志文件。<br />
然后在终端界面输入：</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">tensorboard --logdir<span class="o">=</span>logs/</code></pre></figure>

<p>接着在浏览器里输入http://localhost:6006/ 就可以看到我们设置的变量和一些值得变化了：</p>

<p>如传入nn的一些参数可以在Text一栏可以观察到：</p>

<p><img src="/img/post-reinforcement/re15.jpg" alt="" /></p>

<p>比如loss损失函数的变化：</p>

<p><img src="/img/post-reinforcement/re16.jpg" alt="" /></p>


    </div>
  </div>

  <style>
  .post-nav {
    overflow: hidden;
    /* margin-top: 60px; */
    padding: 12px;
    white-space: nowrap;
    /* border-top: 1px solid #eee; */
  }

  .post-nav-item {
    display: inline-block;
    width: 50%;
    white-space: normal;
  }

  .post-nav-item a {
    position: relative;
    display: inline-block;
    line-height: 25px;
    font-size: 14px;
    color: #555;
    border-bottom: none;
  }

  .post-nav-item a:hover {
    color: #222;
    font-weight: bold;
    border-bottom: none;
  }

  .post-nav-item a:active {
    top: 2px;
  }

  .post-nav-item a:before,
  .post-nav-item a:after {
    display: inline-block;
    width: 16px;
    height: 25px;
    vertical-align: top;
    opacity: 0.4;
    background-size: 16px;
  }

  .post-nav-none a:none {
    content: ' ';
    background-size: 8px;
  }

  .post-nav-none a:hover:none {
    opacity: 1;
  }

  .post-nav-prev a:before {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjE4LjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLWxlZnQiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIxOC41MDAwMDAsIDkwLjAwMDAwMCkiPjxwYXRoIGQ9Ik03LjQsMS40IEw2LDAgTC04Ljg4MTc4NDJlLTE2LDYgTDYsMTIgTDcuNCwxMC42IEwyLjgsNiBMNy40LDEuNCBaIiBpZD0iU2hhcGUiLz48L2c+PC9nPjwvZz48L3N2Zz4=") no-repeat 0 50%;
    background-size: 8px;
  }

  .post-nav-prev a:hover:before {
    opacity: 1;
  }

  .post-nav-next {
    text-align: right;
  }

  .post-nav-next a:after {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjYwLjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLXJpZ2h0IiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgyNjAuNTAwMDAwLCA5MC4wMDAwMDApIj48cGF0aCBkPSJNMSwwIEwtMC40LDEuNCBMNC4yLDYgTC0wLjQsMTAuNiBMMSwxMiBMNyw2IEwxLDAgWiIgaWQ9IlNoYXBlIi8+PC9nPjwvZz48L2c+PC9zdmc+") no-repeat 100% 50%;
    background-size: 8px;
  }

  .post-nav-next a:hover:after {
    opacity: 1;
  }
</style>



<div class="post-nav">

  
  <div class="post-nav-prev post-nav-item">
    
    <a href="/blog/2018/gameai/"> 腾讯、网易都入局的游戏+AI，想象力有多大[转载]</a>
  </div>
  

  
  <div class="post-nav-next post-nav-item">
    
    <a href="/blog/2018/reinforcement2/"> 强化学习-游戏AI Trainning (二)</a>
  </div>
  

</div>

</article>
    </div>

    <footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="https://twitter.com/penghuailiang" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="https://www.zhihu.com/people/huailiangpenguin" target="_blank"><i class="icon icon-zhihu"></i></a></li>
  <li><a href="https://www.facebook.com/profile.php?id=100004290725320" target="_blank"><i class="icon icon-facebook"></i></a></li>
  <li><a href="https://weibo.com/6212299692/profile?topnav=1&wvr=6" target="_blank"><i class="icon icon-sina"></i></a></li>
  <li><a href="https://www.linkedin.com/in/penghuailiang/" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="https://github.com/huailiang" target="_blank"><i class="icon icon-github"></i></a></li>
</ul>
    <p class="txt-medium-gray">
      <small>&copy;2019 All rights reserved. </small>
    </p>
  </div>
</footer>
    <a href="https://github.com/huailiang" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#337ab7; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script>
      $(document).ready(function () {
        $('.sliding-panel-button,.sliding-panel-fade-screen,.sliding-panel-close').on('click touchstart', function (e) {
          $('.sliding-panel-content,.sliding-panel-fade-screen').toggleClass('is-visible');
          e.preventDefault();
        });
      });
    </script>
  </main>
</body>

</html>