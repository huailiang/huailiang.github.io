<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>游戏风格转换</title>
  <meta name="description"
    content="  如今人工智能大行其道， 其中在图像、影音处理中方法颇多。 本文介绍一种游戏中图像风格转换的例子，训练采用Tensorflow-GAN的方式，运行时在Unity引擎使用compute shader实现了跟tensorflow中一样的前向传播的网络来生成转换后的风格。对应Tensorflow环境中的Generat...">

  <link rel="shortcut icon" href="/img/favicon.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://huailiang.github.io/blog/2019/nnstyle/">

</head>

<body>
  <main>
    <header class="site-header">
  <div class="container">
    <h1><a href="/">Hom<span>e</span></a></h1>

    <button type="button" class="sliding-panel-button">
      <span></span>
      <span></span>
      <span></span>
    </button>

    <nav class="navbar sliding-panel-content">
      <ul>
        
        <li><a href="/about" title="About">About</a>
        </li>
        
        <li><a href="/blog" title="Blog">Blog</a>
        </li>
        
       <li><a href="/category/" target="_blank"><i class="icon icon-feed"></i></a></li>
      </ul>
    </nav>
  </div>
</header>

<div class="sliding-panel-fade-screen"></div>
    <script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });

</script>
    <div class="container">
      <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">游戏风格转换</h1>
      <p class="post-meta">May 2, 2019 •
        Huailiang</p>
    </header>

    <div class="post-content">
      <blockquote>
  <p>如今人工智能大行其道， 其中在图像、影音处理中方法颇多。 本文介绍一种游戏中图像风格转换的例子，训练采用Tensorflow-GAN的方式，运行时在Unity引擎使用compute shader实现了跟tensorflow中一样的前向传播的网络来生成转换后的风格。对应Tensorflow环境中的Generator。 对应到github地址点击<a href="https://github.com/huailiang/nnStyle">这里</a>, 实现的效果如下图, 转换风格的图像我们通过一张RT渲染到右下角：</p>
</blockquote>

<center>
    <img src="/img/post-tf/show.gif" height="480" />
</center>
<p><br /><br /></p>

<h2 id="训练集">训练集</h2>

<p>我们在python-tensorflow中实现了一种反向传播网络，我们使用Auto-Encoder替代GAN中Generator，用以生成风格化的图像， 而在Discriminator来鉴别图像。</p>

<p>训练中采用的训练集是微软的coco的<a href="http://mscoco.org">dataset</a>, 下载转换风格<a href="https://hcicloud.iwr.uni-heidelberg.de/index.php/s/NcJj2oLBTYuT1tf">图片集</a>.</p>

<p><img src="/img/post-tf/style1.jpeg" alt="" /></p>

<p>我们在训练Discriminator的时候， 评估损失函数，丢给Discriminator风格化的图片尽量大， coco训练集的图片因为是假的， 我们去使其输出的值尽量小， 通过gennerator的图片，也是假的，我们也使其值也越小。</p>

<p>在训练Genenrator评估其损失函数的时候， generator的图片丢给Discriminator，尽量和真实的风格图片尽量接近，因此我们使其输出的值也要越大。</p>

<p>这里定义content image的内容的损失，方法采用的如<a href="https://arxiv.org/abs/1807.10201">paper</a> 均值池化提取特征后， 比较方差的差异。</p>

<p>定义style featur的损失， 方法采用把generator生成的图片和原始输入的style image分别带入encoder做绝对值的差。</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># Image loss.</span>
<span class="bp">self</span><span class="o">.</span><span class="n">img_loss_photo</span> <span class="o">=</span> <span class="n">mse_criterion</span><span class="p">(</span>
    <span class="n">transformer_block</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_photo</span><span class="p">),</span> <span class="n">transformer_block</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_photo</span><span class="p">))</span>
<span class="bp">self</span><span class="o">.</span><span class="n">img_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">img_loss_photo</span>

<span class="c"># Features loss.</span>
<span class="bp">self</span><span class="o">.</span><span class="n">feature_loss_photo</span> <span class="o">=</span> <span class="n">abs_criterion</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_photo_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_photo_features</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">feature_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_loss_photo</span></code></pre></figure>

<p>Tensorboard 里观察loss的变化：</p>

<p><img src="/img/post-tf/loss.jpg" alt="" /></p>

<h2 id="运行时">运行时</h2>

<p>在unity中， 我们使用compute shader实现了一套跟TensorFlow中的相同的前向传播网络。 这里只是实现了Encoder和Decoder， 并没有实现Discriminator, 因为只有训练的时候用到了Discriminator.</p>

<p>在网络层中 Batch-normal由于为了求整体的均值和方差， 需要遍历当前层每个深度的layer，需要使用归纳算法-reduce, 使计算效率时间复杂度由n变成logn, 所以设计网络的时候尽量减少了类似的操作，在<a href="i4">CompVis</a>的设计网络中，decoder使用了九个残差模块， 为了性能我们这里减少到了一个。 参数规模也由大概48M减少到12M， 却实现了类似的效果。</p>

<p>由于受限于compute shader的语法， 我们在定义thread group时候， thread的大小不能超过1024，thread-z不能超过64， group的组成是vec3的格式， 而且需要是32或者64的倍数， 因此我们在设计网络的时候，每一个的layer尽量去靠近这些特性， 致使GPU发挥出最大的性能。 （CS5 group thread个数最多是1024， 而CS4最多支持到512，Apple的平台最多支持到CS4，这里需要注意下）。</p>

<p>compute shader里网络的参数规模如下图所示：</p>

<center>
    <img src="/img/post-tf/style3.jpg" height="200" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="/img/post-tf/style4.jpg" height="200" />
</center>

<p>简化过后的，对应的tensorboard里的版本：</p>

<center>
    <img src="/img/post-tf/encoder.jpg" height="480" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="/img/post-tf/decoder.jpg" height="480" />
</center>

<p>在设计的CNN网络层中， 卷积核大都是3x3的， 所以这里我们定义了一个Matrix3x3.cs在csharp中， 这样给StructureBuffer传递数据的时候，一次性传递过去。</p>

<h2 id="数据导出">数据导出</h2>

<p>tensorflow 训练数据集有自己的序列化方式，大概是protobuf,  google也提供了一套api, 去获取里面的张量Tensor。</p>

<p>通过训练集之后导出的checkpoint文件大小超过一个G，如果把这么庞大的参数文件导入到unity中所开销的内存空间是无法想象的。</p>

<p>通过遍历checkpoint发现所有的tensor发现， 每一层layer， 网络中的每个参数都对应了两个Adam对象，所以我们写了一套工具，导出数据的时候过滤掉Adam对象，使其大小减小到之前的1/3.</p>

<p>在上面提到，discriamtor只在训练的时候用到，而且其参数规模远超generator, 这里我们在导出的时候也需要对其过滤掉。</p>

<p>通过上面的操作我们导出的参数规模大概是48M，由于可以去掉预算复杂且效率不高的残差网络模块，参数规模进一步缩小到十几兆。 这还是我们采用float存储的格式， 如果对精度要求不高， 采用half的数据格式 5M-6M之间， 我觉得这个大小都手机平台还是可以接受的。</p>

<h2 id="数据调试">数据调试</h2>

<p>为了清晰的看出Tensorflow Session里的每一层数据， 我们同时在python和unity环境中定义了printf 和 printh 函数， 分别用来输出二三维和一二维数据到控制台。  也可以检测python环境和unity环境运算结果是否具有一致性。</p>

<p>为了看出神经网络中的参数整体的相关性， 这里通过工具导出Auto-Encoder每一层里的数据（.bytes 二进制文件）， 然后在unity导入数据。通过Unity-Editor工具，我们对数据HXW前两维的数据生成一张Texture上， 在生成数据之前，我们通过激励函数sigmod, 使全部数据normalized到取件[0,1]之间。 之后，在通过一个slider表示第三维度的数据，通过拖拽slider的进度， 就可以看到不同depth的数据整体相关情况了。</p>

<center>
    <img src="/img/post-tf/style2.jpg" height="480" />
</center>
<p><br /></p>

<p>我们通过一张图片看到， ganerator前期encoder前期更多的学到的是形状信息， 后期decoder学到的更多的是风格相关的信息。</p>

<h2 id="性能分析">性能分析</h2>

<ol>
  <li><b>参数规模:</b><br /></li>
</ol>

<p>在导出参数二进制文件到unity中的时候， 我们在python 控制台输出了参数规模大小约88M， 在过滤掉只在训练集里用到的参数， 导出的文件大概是9.7M。 在项目的fast分支， 我们删除了一些不重要的layer， 精简网络之后，参数规模占用23M， 过滤掉只在训练集里用到的参数， 导出的文件大小只有600K多一些了， 相比较于Tensorflow中Checkpoint文件中多达一个多G的参数规模，我们在运行时参数规模精简了1000多倍。（tensorflow checkpoint文件采用了protobuf格式的数据存储，我们这里纯是二进制， 因为protobuf虽然也是二进制， 但是会记一些tag之类的版本兼容相关的信息， 另外tensorflow还会存discriminator的信息，这块实现比较复杂，但只会出现在训练集里）。</p>

<ol>
  <li><b>帧率:</b><br /></li>
</ol>

<p>当我们使用完整的pix2pixHD去实现的时候， PC上大概能跑3FPS, MAC上大概还不到1FPS。 之后在fast分支， 我们去掉了一些不重要的网络层，PC上大概能跑到29FPS，在文章的效果图里可以看到对应的效果。</p>

<p>在手机上， 我们对模糊效果的处理，往往为了性能会控制采样点的数量，也就是卷积核的大小。当采样点超过9个的话， 性能相对来说就会有一定的下降。 如果按此来比对基于深度神经网络方式实现的风格转换的做法时，那么这里采样的点岂止9个，即使fast上的实现方式，每个像素的采样规模也是模糊处理的几十倍， 所以帧率低也是有一定道理的。 在fast分支处理的时候， 我们尽量删除了那些depth标记深的layer, 因为cnn的时候要对每一层累加， 归纳reduce算法对于GPU来说，因为要遍历全局，时间复杂度最快也是logn。</p>


    </div>
  </div>

  <style>
  .post-nav {
    overflow: hidden;
    /* margin-top: 60px; */
    padding: 12px;
    white-space: nowrap;
    /* border-top: 1px solid #eee; */
  }

  .post-nav-item {
    display: inline-block;
    width: 50%;
    white-space: normal;
  }

  .post-nav-item a {
    position: relative;
    display: inline-block;
    line-height: 25px;
    font-size: 14px;
    color: #555;
    border-bottom: none;
  }

  .post-nav-item a:hover {
    color: #222;
    font-weight: bold;
    border-bottom: none;
  }

  .post-nav-item a:active {
    top: 2px;
  }

  .post-nav-item a:before,
  .post-nav-item a:after {
    display: inline-block;
    width: 16px;
    height: 25px;
    vertical-align: top;
    opacity: 0.4;
    background-size: 16px;
  }

  .post-nav-none a:none {
    content: ' ';
    background-size: 8px;
  }

  .post-nav-none a:hover:none {
    opacity: 1;
  }

  .post-nav-prev a:before {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjE4LjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLWxlZnQiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIxOC41MDAwMDAsIDkwLjAwMDAwMCkiPjxwYXRoIGQ9Ik03LjQsMS40IEw2LDAgTC04Ljg4MTc4NDJlLTE2LDYgTDYsMTIgTDcuNCwxMC42IEwyLjgsNiBMNy40LDEuNCBaIiBpZD0iU2hhcGUiLz48L2c+PC9nPjwvZz48L3N2Zz4=") no-repeat 0 50%;
    background-size: 8px;
  }

  .post-nav-prev a:hover:before {
    opacity: 1;
  }

  .post-nav-next {
    text-align: right;
  }

  .post-nav-next a:after {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjYwLjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLXJpZ2h0IiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgyNjAuNTAwMDAwLCA5MC4wMDAwMDApIj48cGF0aCBkPSJNMSwwIEwtMC40LDEuNCBMNC4yLDYgTC0wLjQsMTAuNiBMMSwxMiBMNyw2IEwxLDAgWiIgaWQ9IlNoYXBlIi8+PC9nPjwvZz48L2c+PC9zdmc+") no-repeat 100% 50%;
    background-size: 8px;
  }

  .post-nav-next a:hover:after {
    opacity: 1;
  }
</style>



<div class="post-nav">

  
  <div class="post-nav-prev post-nav-item">
    
    <a href="/blog/2019/cg/"> CG/hlsl 内置函数</a>
  </div>
  

  
  <div class="post-nav-next post-nav-item">
    
    <a href="/blog/2019/inout/"> CG中修饰符in out inout的使用</a>
  </div>
  

</div>

</article>
    </div>

    <footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="https://twitter.com/penghuailiang" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="https://www.zhihu.com/people/huailiangpenguin" target="_blank"><i class="icon icon-zhihu"></i></a></li>
  <li><a href="https://www.facebook.com/profile.php?id=100004290725320" target="_blank"><i class="icon icon-facebook"></i></a></li>
  <li><a href="https://weibo.com/6212299692/profile?topnav=1&wvr=6" target="_blank"><i class="icon icon-sina"></i></a></li>
  <li><a href="https://www.linkedin.com/in/penghuailiang/" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="https://github.com/huailiang" target="_blank"><i class="icon icon-github"></i></a></li>
</ul>
    <p class="txt-medium-gray">
      <small>&copy;2023 All rights reserved. </small>
    </p>
  </div>
</footer>
    <a href="https://github.com/huailiang" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#337ab7; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script>
      $(document).ready(function () {
        $('.sliding-panel-button,.sliding-panel-fade-screen,.sliding-panel-close').on('click touchstart', function (e) {
          $('.sliding-panel-content,.sliding-panel-fade-screen').toggleClass('is-visible');
          e.preventDefault();
        });
      });
    </script>
  </main>
</body>

</html>