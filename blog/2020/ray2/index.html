<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>光线追踪-降噪</title>
  <meta name="description"
    content="  图像去噪是非常基础也是非常必要的研究，去噪常常在更高级的图像处理之前进行，是图像处理的基础。可惜的是，目前去噪算法并没有很好的解决方案，实际应用中，更多的是在效果和运算复杂度之间求得一个平衡。">

  <link rel="shortcut icon" href="/img/favicon.ico">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://huailiang.github.io/blog/2020/ray2/">

</head>

<body>
  <main>
    <header class="site-header">
  <div class="container">
    <h1><a href="/">Hom<span>e</span></a></h1>

    <button type="button" class="sliding-panel-button">
      <span></span>
      <span></span>
      <span></span>
    </button>

    <nav class="navbar sliding-panel-content">
      <ul>
        
        <li><a href="/about" title="About">About</a>
        </li>
        
        <li><a href="/blog" title="Blog">Blog</a>
        </li>
        
       <li><a href="/category/" target="_blank"><i class="icon icon-feed"></i></a></li>
      </ul>
    </nav>
  </div>
</header>

<div class="sliding-panel-fade-screen"></div>
    <script type="text/javascript" src="/js/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });

</script>
    <div class="container">
      <article role="article" class="post">

  <div class="card">
    <header class="post-header">
      <h1 class="post-title">光线追踪-降噪</h1>
      <p class="post-meta">Jan 6, 2020 •
        Huailiang</p>
    </header>

    <div class="post-content">
      <blockquote>
  <p>图像去噪是非常基础也是非常必要的研究，去噪常常在更高级的图像处理之前进行，是图像处理的基础。可惜的是，目前去噪算法并没有很好的解决方案，实际应用中，更多的是在效果和运算复杂度之间求得一个平衡。</p>
</blockquote>

<h3 id="噪声模型">噪声模型</h3>

<p>图像中噪声的来源有许多种，这些噪声来源于图像采集、传输、压缩等各个方面。噪声的种类也各不相同，比如椒盐噪声，高斯噪声等，针对不同的噪声有不同的处理算法。</p>

<p>对于输入的带有噪声的图像v(x)，其加性噪声可以用一个方程来表示：</p>

<script type="math/tex; mode=display">v(x) = u(x) + \eta (x),\quad x \in \Omega ,</script>

<p>其中u(x)是原来没有噪声的图像。x是像素集合，η(x)是加项噪声项，代表噪声带来的影响。Ω是像素的集合，也就是整幅图像。从这个公式可以看出，噪声是直接叠加在原始图像上的，这个噪声可以是椒盐噪声、高斯噪声。理论上来说，如果能够精确地获得噪声，用输入图像减去噪声就可以恢复出原始图像。但现实往往很骨感，除非明确地知道噪声生成的方式，否则噪声很难单独求出来。</p>

<p>工程上，图像中的噪声常常用高斯噪声$N(μ,σ^2)$来近似表示，其中$μ=0，σ^2$是噪声的方差，$σ^2$越大，噪声越大。一个有效的去除高斯噪声的方式是图像求平均，对N幅相同的图像求平均的结果将使得高斯噪声的方差降低到原来的N分之一，现在效果比较好的去噪算法都是基于这一思想来进行算法设计。</p>

<iframe src="//player.bilibili.com/player.html?aid=29458366&amp;cid=51215573&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="700" height="460"> </iframe>

<h3 id="为什么光线追踪会出现噪点">为什么光线追踪会出现噪点</h3>

<p>因为光线追踪，确切地说是路径追踪(Path Tracing)本质上是在解渲染方程，一个积分方程。</p>

<script type="math/tex; mode=display">L(x, \, \vec \omega_{o}) = L_e(x, \, \vec \omega_{o}) + \int_{\Omega}{f_r(x, \, \vec \omega_{i}, \, \vec \omega_{o}) \, (\vec \omega_{i} \cdot \vec n) \, L(x, \, \vec \omega_{i}) \, d\vec \omega_{i}}</script>

<p>使用蒙特卡洛采样， 将上面积分式转换为离散的表达式:</p>

<script type="math/tex; mode=display">L(x, \, \vec \omega_{o}) \approx L_e(x, \, \vec \omega_{o}) + \frac{1}{N} \sum_{n=0}^{N}{2 \pi \, f_r(x, \, \vec \omega_{i}, \, \vec \omega_{o}) \, (\vec \omega_{i} \cdot \vec n) \, L(x, \, \vec \omega_{i})}</script>

<p>当然，引入这个方法，如果采样数量不够多，会造成光照贡献量与实际值偏差依然会很大，形成噪点（即上式中N比较小）。随着采样数量的增加，局部估算越来越接近实际光照积分，噪点逐渐消失（下图）。</p>

<p><img src="/img/post-ml/ray1.jpg" alt="" /></p>

<p><em>从左到右分别对应的每个象素采样为1、16、256、4096、65536</em></p>

<h2 id="峰值信噪比">峰值信噪比</h2>

<p>PSNR是<em>Peak Signal to Noise Ratio</em>的缩写，即峰值信噪比，是一种评价图像的客观标准，它具有局限性，一般是用于最大值信号和背景噪音之间的一个工程项目。</p>

<p>psnr一般是用于最大值信号和背景噪音之间的一个工程项目。通常在经过影像压缩之后，输出的影像都会在某种程度与原始影像不同。为了衡量经过处理后的影像品质，我们通常会参考PSNR值来衡量某个处理程序能否令人满意。它是原图像与被处理图像之间的均方误差相对于$(2^n-1)^2$的对数值(信号最大值的平方，n是每个采样值的比特数)，它的单位是dB。</p>

<p>给定一个大小为 m×n 的干净图像 I 和噪声图像 K，均方误差 (MSE) 定义为：</p>

<script type="math/tex; mode=display">MSE = \frac{1}{mn}\sum_{i=0}^{m-1}\sum_{j=0}^{n-1}[I(i, j)-K(i,j)]^2</script>

<p>数学公式如下：</p>

<script type="math/tex; mode=display">PSNR = 10 \cdot log_{10}(\frac{(2^n-1)^2}{MSE})</script>

<p>一般地，针对 uint8 数据，最大像素值为 255,；针对浮点型数据，最大像素值为 1。n为每像素的比特数，一般取8，即像素灰阶数为256. PSNR的单位是dB，数值越大表示失真越小。</p>

<p>上面是针对灰度图像的计算方法，如果是彩色图像，通常有三种方法来计算。</p>
<ul>
  <li>分别计算 RGB 三个通道的 PSNR，然后取平均值。</li>
  <li>计算 RGB 三通道的 MSE ，然后再除以 3 。</li>
  <li>将图片转化为 YCbCr 格式，然后只计算 Y 分量也就是亮度分量的 PSNR。</li>
</ul>

<div class="language-py highlighter-rouge"><pre class="highlight"><code><span class="c"># im1 和 im2 都为灰度图像，uint8 类型</span>

<span class="c"># method 1</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">im1</span> <span class="o">-</span> <span class="n">im2</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span>
<span class="n">psnr</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="mi">255</span> <span class="o">/</span> <span class="n">mse</span><span class="p">)</span>

<span class="c"># method 2</span>
<span class="n">psnr</span> <span class="o">=</span> <span class="n">skimage</span><span class="o">.</span><span class="n">measure</span><span class="o">.</span><span class="n">compare_psnr</span><span class="p">(</span><span class="n">im1</span><span class="p">,</span> <span class="n">im2</span><span class="p">,</span> <span class="mi">255</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="数据集">数据集</h3>

<p>可以使用<a href="https://github.com/tunabrain/tungsten">Tungsten</a>引擎来生成， 也可以去开源的站点去<a href="https://benedikt-bitterli.me/nfor/denoising-data.zip">下载</a>。 下面主要介绍使用引擎生成数据集的方式。</p>

<p>Tungsten是一个基于物理的渲染器，最初为ETH年的年度渲染竞赛编写。 它通过对渲染方程的无偏积分来模拟通过任意几何的全光传输。 Tungsten支持各种光传输算法，如双向路径跟踪BRDF、渐进光子映射、空间城市光传输等。Tungsten是用C++11编写的，利用了几何交叉库embree的高性能。 Tungsten充分利用多核系统，并通过频繁的基准和优化来提供良好的性能。 运行渲染器至少需要SSE3支持。</p>

<p><img src="/img/post-ml/ray3.jpg" alt="" /></p>

<p>编译生成引擎， 需要你本地已经安装了GCC， 下载然后make:</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code>git clone https://github.com/tunabrain/tungsten.git
./setup_builds.sh
<span class="nb">cd </span>build/release
make
</code></pre>
</div>

<p>然后添加Tungsten 到环境变量PATH里去，（Mac系统保存环境变量在.profle文件)</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code><span class="nb">echo</span> <span class="s1">'export PATH="&lt;tungsten-release-dir&gt;":$PATH'</span> &gt;&gt; ~/.bashrc
<span class="nb">source</span> ~/.bashrc
</code></pre>
</div>

<p>在终端里敲命令，看是否配置成功：</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code>tungsten -v
</code></pre>
</div>

<p>下载场景贴图(大概860张建筑贴图)：</p>

<div class="language-sh highlighter-rouge"><pre class="highlight"><code><span class="nb">cd </span>data <span class="o">&amp;&amp;</span> mkdir scenes
wget https://benedikt-bitterli.me/resources/tungsten/bathroom.zip
unzip bathroom.zip -d scenes
rm <span class="k">*</span>.zip
</code></pre>
</div>

<p><a href="https://github.com/joeylitalien/noise2noise-pytorch">render.py</a>生成训练集：</p>
<div class="language-py highlighter-rouge"><pre class="highlight"><code><span class="n">python3</span> <span class="n">render</span><span class="o">.</span><span class="n">py</span> \
  <span class="o">--</span><span class="n">scene</span><span class="o">-</span><span class="n">path</span> <span class="o">../</span><span class="n">data</span><span class="o">/</span><span class="n">scenes</span><span class="o">/</span><span class="n">bathroom</span><span class="o">/</span><span class="n">scene</span><span class="o">.</span><span class="n">json</span> \
  <span class="o">--</span><span class="n">spp</span> <span class="mi">8</span> \
  <span class="o">--</span><span class="n">nb</span><span class="o">-</span><span class="n">renders</span> <span class="mi">48</span> \
  <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="nb">dir</span> <span class="o">../</span><span class="n">data</span><span class="o">/</span><span class="n">mc</span><span class="o">/</span><span class="n">train</span> \
  <span class="o">--</span><span class="n">hdr</span><span class="o">-</span><span class="n">targets</span>
</code></pre>
</div>

<p>Tungsten生成的mc-noised 和cleaned 图像对比：</p>

<p><img src="/img/post-ml/ray4.jpg" alt="" /></p>

<p>Unity 生成的mc-noised 和 cleaned的图像对比：</p>

<p><img src="/img/post-ml/ray5.jpg" alt="" /></p>

<h2 id="nl-means算法">NL-Means算法</h2>

<p>NL-Means的全称是：Non-Local Means，直译过来是非局部平均，在2005年由Baudes提出，该算法使用自然图像中普遍存在的冗余信息来去噪声。与常用的双线性滤波、中值滤波等利用图像局部信息来滤波不同的是，它利用了整幅图像来进行去噪，以图像块为单位在图像中寻找相似区域，再对这些区域求平均，能够比较好地去掉图像中存在的高斯噪声。NL-Means的滤波过程可以用下面公式来表示：</p>

<script type="math/tex; mode=display">\tilde u(x) = \sum\limits_{y \in {\Omega _x}} {w(x,y)v(y)}</script>

<p>在这个公式中，w(x,y)是一个权重，表示在原始图像v中，像素 x 和像素 y 的相似度。这个权重要大于0，同时，权重的和为1，用公式表示是这样：</p>

<script type="math/tex; mode=display">w(x,y) > 0\quad and\quad \sum\limits_{y \in {\Omega _x}} {w(x,y) = 1}, \quad \forall x \in \Omega ,y \in {\Omega _x}</script>

<p>Ωx是像素 x 的邻域。这个公式可以这样理解：对于图像中的每一个像素 x ，去噪之后的结果等于它邻域中像素 y 的加权和，加权的权重等于 x 和 y 的相似度。这个邻域也称为搜索区域，搜索区域越大，找到相似像素的机会也越大，但同时计算量也是成指数上升。在提出这个算法的文献中，这个区域是整幅图像！导致的结果是处理一幅512x512大小的图像，最少也得几分钟。</p>

<p>衡量像素相似度的方法有很多，最常用的是根据两个像素的亮度值的差的平方来估计。但因为有噪声的存在，单独的一个像素并不可靠。对此解决方法是，考虑它们的邻域，只有邻域相似度高才能说这两个像素的相似度高。衡量两个图像块的相似度最常用的方法是计算他们之间的欧氏距离：</p>

<script type="math/tex; mode=display">w(x,y) = \frac{1} {n(x)} \exp ({\frac{\| {V(x) - V(y)}\|_{2,a}^2}  {h^2}})</script>

<p>其中： n(x) 是一个归一化的因子，是所有权重的和，对每个权重除以该因子后，使得权重满足和为1的条件。 h&gt;0 是滤波系数，控制指数函数的衰减从而改变欧氏距离的权重。 V(x) 和 V(y) 代表了像素 x 和像素 y 的邻域，这个邻域常称为块(Patch)邻域。块邻域一般要小于搜索区域。${|{V(x) - V(y)}|_{2,a}^2}$ 是两个邻域的高斯加权欧式距离。其中 a&gt;0 是高斯核的标准差。在求欧式距离的时候，不同位置的像素的权重是不一样的，距离块的中心越近，权重越大，距离中心越远，权重越小，权重服从高斯分布。实际计算中考虑到计算量的问题，常常采用均匀分布的权重。</p>

<center><img src="/img/post-ml/ray6.jpg" /></center>

<p>如上图所示，p为去噪的点，因为q1和q2的邻域与p相似，所以权重w(p,q1)和w(p,q2)比较大，而邻域相差比较大的点q3的权重值w(p,q3)很小。如果用一幅图把所有点的权重表示出来，那就得到下面这些权重图：</p>

<p><img src="/img/post-ml/ray7.jpg" alt="" /></p>

<p>这6组图像中，左边是原图，中心的白色色块代表了像素 x 块邻域，右边是计算出来的权重 w(x,y) 图，权重范围从0（黑色）到1（白色）。这个块邻域在整幅图像中移动，计算图像中其他区域跟这个块的相似度，相似度越高，得到的权重越大。最后将这些相似的像素值根据归一化之后的权重加权求和，得到的就是去噪之后的图像了。</p>

<p>这个算法参数的选择也有讲究，一般而言，考虑到算法复杂度，搜索区域大概取21x21，相似度比较的块的可以取7x7。实际中，常常需要根据噪声来选取合适的参数。当高斯噪声的标准差 σ 越大时，为了使算法鲁棒性更好，需要增大块区域，块区域增加同样也需要增加搜索区域。同时，滤波系数 h 与 σ 正相关：h=kσ，当块变大时，k 需要适当减小。</p>

<p>NL-Means算法的复杂度跟图像的大小、颜色通道数、相似块的大小和搜索框的大小密切相关，设图像的大小为N×N，颜色通道数为Nc，块的大小为k×k，搜索框的大小为n×n，那么算法复杂度为：O(N2Nck2n2)。对512×512的彩色图像而言，设置k=7，n=21，OpenCV在使用了多线程的情况下，处理一幅图像所需要的时间需要几十秒。虽然有人不断基于这个算法进行改进、提速，但离实时处理还是比较远。</p>

<p>最后来看一下这个算法的去噪效果[3]：</p>

<p><img src="/img/post-ml/ray8.jpg" alt="" /></p>

<h3 id="bm3d算法">BM3D算法</h3>

<p>BM3D（Block-matching and 3D filtering，3维块匹配滤波）可以说是当前效果最好的算法之一。该算法的思想跟NL-Means有点类似，也是在图像中寻找相似块的方法进行滤波，但是相对于NL-Means要复杂得多，理解了NL-Means有助于理解BM3D算法。BM3D算法总共有两大步骤，分为基础估计（Step1）和最终估计（Step2）：</p>

<p><img src="/img/post-ml/ray9.jpg" alt="" /></p>

<p>在这两大步中，分别又有三小步：相似块分组（Grouping），协同滤波（Collaborative Filtering）和聚合（Aggregation）。上面的算法流程图已经比较好地将这一过程表示出来了，只需要稍加解释。</p>

<p><strong>Stpe1：基础估计</strong></p>

<p>(1) Grouping：有了NL-Means的基础，寻找相似块的过程很容易理解。首先在噪声图像中选择一些k×k 大小的参照块（考虑到算法复杂度，不用每个像素点都选参照块，通常隔3个像素为一个不长选取，复杂度降到1/9），在参照块的周围适当大小（n×n）的区域内进行搜索，寻找若干个差异度最小的块，并把这些块整合成一个3维的矩阵，整合的顺序对结果影响不大。同时，参照块自身也要整合进3维矩阵，且差异度为0。寻找相似块这一过程可以用一个公式来表示：</p>

<script type="math/tex; mode=display">G(P) = \{ Q:d(P,Q) \le {\tau ^{step1}}\} .</script>

<p>d(P,Q)代表两个块之间的欧式距离。最终整合相似块获得的矩阵就是流程图Step1中左下角的蓝色R矩阵。</p>

<center><img src="/img/post-ml/ray10.jpg" /></center>

<p>(2) Collaborative Filtering：形成若干个三维的矩阵之后，首先将每个三维矩阵中的二维的块（即噪声图中的某个块）进行二维变换，可采用小波变换或DCT变换等，通常采用小波BIOR1.5。二维变换结束后，在矩阵的第三个维度进行一维变换，通常为阿达马变换（Hadamard Transform）。变换完成后对三维矩阵进行硬阈值处理，将小于阈值的系数置0，然后通过在第三维的一维反变换和二维反变换得到处理后的图像块。这一过程同样可以用一个公式来表达：</p>

<script type="math/tex; mode=display">Q(P) = T_{3Dhard}^{ - 1}(\gamma (T_{3Dhard}(Q(P)))).</script>

<p>在这个公式中，二维变换和一维变换用一个T3Dhard 来表示。γ是一个阈值操作：</p>

<script type="math/tex; mode=display">\gamma (x) =
\left\{
\begin{aligned}
0 \quad if |x| \le {\lambda _{3D}\sigma }  \\
x \quad \quad otherwise
\end{aligned}
\right.</script>

<p>σ是噪声的标准差，代表噪声的强度。</p>

<p>(3) Aggregation：此时，每个二维块都是对去噪图像的估计。这一步分别将这些块融合到原来的位置，每个像素的灰度值通过每个对应位置的块的值加权平均，权重取决于置0的个数和噪声强度。</p>

<p><strong>Step2：最终估计</strong></p>

<p>(1) Grouping：第二步中的聚合过程与第一步类似，不同的是，这次将会得到两个三维数组：噪声图形成的三维矩阵Qbasic(P)和基础估计结果的三维矩阵Q(P)。</p>

<p>(2) Collaborative Filtering：两个三维矩阵都进行二维和一维变换，这里的二维变换通常采用DCT变换以得到更好的效果。用维纳滤波（Wiener Filtering）将噪声图形成的三维矩阵进行系数放缩，该系数通过基础估计的三维矩阵的值以及噪声强度得出。这一过程同样可以用一个公式来表达：</p>

<script type="math/tex; mode=display">Q(P) = T_{3Dwein}^{ - 1}(w_p \cdot {T_{3Dwein}}(Q(P))).</script>

<p>在这个公式中，二维变换和一维变换用一个T3Dwein 来表示。wp是一个维纳滤波的系数：</p>

<script type="math/tex; mode=display">{w_p}(\xi ) = \frac{|\tau_{3D}^{wien}(Q^{basic}(P))(\xi)|^2}{|\tau_{3D}^{wien}(Q^{basic}(P))|^2+{\sigma ^2}}</script>

<p>σ是噪声的标准差，代表噪声的强度。</p>

<p>(3) Aggregation：与第一步中一样，这里也是将这些块融合到原来的位置，只是此时加权的权重取决于维纳滤波的系数和噪声强度。</p>

<p><img src="/img/post-ml/ray11.jpg" alt="" /></p>

<p>经过最终估计之后，BM3D算法已经将原图的噪声显著地去除。可以来看一组结果：</p>

<p><img src="/img/post-ml/ray2.jpg" alt="" /></p>

<p>该算法的主要运算量还是在相似块的搜索与匹配上，在与NL-Means同样大小的相似块和搜索区域的情况下，BM3D的算法复杂度是要高于NL-Means的，应该大概在NL-Means的3倍左右。实时处理是跑不起来了。</p>

<h3 id="两个算法的psnr比较">两个算法的PSNR比较</h3>

<table border="1">
 <tr>
    <th></th>
    <th>NL_Mean</th>
    <th>BM3D</th>
</tr>        
<tr>
    <td>PSNR</td>
    <td>32.09</td>
    <td>33.67</td>
</tr>                
</table>

<p>NL-Means和BM3D可以说是目前效果最好的去噪算法，其中BM3D甚至宣称它可以得到迄今为止最高的PSNR。从最终的结果也可以看出来，BM3D的效果确实要好于NL-Means，噪声更少，能够更好地恢复出图像的细节。</p>

<h2 id="noise2noise">Noise2Noise</h2>

<p><a href="https://arxiv.org/abs/1803.04189">Noise2Noise-Learning Image Restoration without Clean Data</a>在ICML2018上，是图像领域的重要的论文。训练图像去噪不需要无噪的原图像。Noise2Noise的AI系统是基于深度学习算法创建的，并且已利用ImageNet数据集提供的50000张图片进行了强化锻炼。每一张训练图片都是由清晰的高质量原图上随机地加上噪点而创建的。计算机生成的图像和核磁共振（MRI）扫描成像也被用于训练Noise2Noise的AI系统。</p>

<p>如果我们接触过图像（信号）恢复中基于模型（重建）的算法，我们就知道：其难点和麻烦的地方，在于对似然函数（降质模型）和图像先验（稀疏、平滑等）的建模。而CNN很好地解决了这一问题，但需要大量的训练数据，通常是受损输入$x^i$和干净目标$y_i$，并且训练目标是最小化经验损失：</p>

<script type="math/tex; mode=display">\arg\min_{\theta} \sum_i L(f_{\theta}(\hat{x}_i), y_i) \tag{1}</script>

<p>其中，$f_θ$是参数化的映射（a parametric family of mappings），例如CNN。</p>

<p>获取大量干净数据是很困难的。例如，为了获得无噪图像，我们需要长曝光；为了获得MRI图像的完整采样，图像中不能有动态目标等。</p>

<p><strong>点估计</strong></p>

<p>假设我们有一组温度采样数据(y1,y2,…)。我们希望在某种损失度量L下，得到温度估计值z（希望该损失最小）：</p>

<script type="math/tex; mode=display">\arg\min_z \mathbb{E}_y \{L(z,y)\} \tag{2}</script>

<p>如果采用L2损失，那么估计值就是观测值的算术平均：</p>

<script type="math/tex; mode=display">z = \mathbb{E}_y \{ y \} \tag{3}</script>

<p>点估计带有一些统计平均的性质。比如，我们可以简单地对多点采样的温度取平均，得到最终的估计温度。</p>

<p><strong>神经网络算法与点估计的关系</strong></p>

<p>式1表达的是参数预测问题（不是简单地估计值，而是学习一个预测模型，服务千千万万的输入），式2是点估计问题，二者不是一个东西。理想状况下，网络的优化方式如下（提供准确的先验和似然）</p>

<script type="math/tex; mode=display">\arg\min_{\theta} \mathbb{E}_{(x,y)} \{ L(f_{\theta}(x), y) \} = \arg\min_{\theta} \mathbb{E}_x \{ \mathbb{E}_{(y | x)} \{ L(f_{\theta}(x), y) \} \} \tag{4}</script>

<p>上式可以理解为：对于每一个样本$x_i$，都在执行一次点估计。可以理解为：根据观测点y，估计点$z=f_θ(x)$，而估计完成时，参数θ就可以根据z推出（或者说二者本质是一样的）。</p>

<p>当然，这种论证是很粗糙的，但提供给我们一个非常有用的见解。我们考虑超分辨问题：这是一个典型的病态问题，因为高频信息在采样过程中丢掉了，而同一张LR图像可以对应大量的HR图像。<br />
借助上述点估计思想，我们不难理解：神经网络实际上是将这些大量的、可能的HR图像做了一个统计平均（点估计的特性），因此L2范数下超分辨图像常常被过度平滑。既然是统计平均，那么我们可以将干净图像y随意换成其他图像（信号），只要保证期望不变，那么也能得到我们想要的估计值$z=f_θ(x)$（式3），进而得到不变的参数θ（式4）！</p>

<p>换句话说，如果假设噪声零均值（或保证期望仍然是无噪图像的期望），那么我们就可以让神经网络的输入和监督都是有噪图像，学习的参数是一样的！</p>

<script type="math/tex; mode=display">\mathbb{E} \{ \hat{y}_i | \hat{x}_i \} = y = \mathbb{E} \{ y_i | \hat{x}_i \} \tag{5}</script>

<p>其中，$x^i$是有噪图像，$y^i$也是有噪图像（$y_i$不一定要和$x_i$相同），y就是目标干净图像。</p>

<h3 id="nvidia-optix">NVIDIA OptiX</h3>

<p><a href="https://zhuanlan.zhihu.com/p/36429298">NVIDIA OptiX</a> API是基于GPU实现高性能光线追踪的应用程序框架。它为加速光线追踪算法提供了一个简单、递归且灵活的管线。OptiX SDK包含两个可相互独立使用的主要组件：用于渲染器开发的光线追踪引擎和post process管线来处理最终显示的像素。</p>

<p>OptiX使用Iray渲染的成千上万的图像构建了一个神经网络，现在这个学习的数据可以应用到其他光线跟踪图像。 凭借OptiX引擎，从前需要耗费数分钟的软件操作现在只需几毫秒即可完成，从而让设计师能够在行业标准硬件上交互地检查真实场景中的光照、反射、折射以及阴影的播放效果。 NVIDIA®（英伟达™）Quadro和Tesla产品拥有业内最大容量的显存，能够处理最大的数据集，可打造出高可靠性的解决方案，非常适合用于GPU光线追踪。 Quadro还能够提供顶级图形性能，是专业人士在处理图形与光线追踪时的首选产品。</p>

<p><img src="/img/post-ml/ray12.jpg" alt="" /></p>

<p>OptiX AI降噪技术与Quadro GV100和Titan V中的全新NVIDIA Tensor Cores相结合，可提供相当于前代GPU 3倍的性能，并首次实现了无噪声流体交互。</p>

<h2 id="参考资料">参考资料</h2>

<ul>
  <li><a href="https://github.com/tunabrain/tungsten">Tungsten 基于物理的渲染器</a></li>
  <li><a href="https://www.embree.org/">几何交叉库 embree</a></li>
  <li><a href="https://baike.baidu.com/item/psnr/2925132?fr=aladdin">峰值信噪比, PSNR, 百科</a></li>
  <li><a href="https://arxiv.org/abs/1803.04189">Learning Image Restoration without Clean Data, arxiv</a></li>
  <li><a href="https://github.com/joeylitalien/noise2noise-pytorch">Learning Image Restoration without Clean Data, pytorch, github</a></li>
  <li><a href="https://github.com/NVlabs/noise2noise">Learning Image Restoration without Clean Data, tensorflow, github</a></li>
  <li><a href="https://developer.nvidia.com/optix">NVIDIA Optix Instro</a></li>
  <li><a href="https://baike.baidu.com/item/v-ray/3745231?fr=aladdin">v-ray instro, 百科</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/36429298">基于RTX的NVIDIA OptiX光线追踪</a></li>
</ul>


    </div>
  </div>

  <style>
  .post-nav {
    overflow: hidden;
    /* margin-top: 60px; */
    padding: 12px;
    white-space: nowrap;
    /* border-top: 1px solid #eee; */
  }

  .post-nav-item {
    display: inline-block;
    width: 50%;
    white-space: normal;
  }

  .post-nav-item a {
    position: relative;
    display: inline-block;
    line-height: 25px;
    font-size: 14px;
    color: #555;
    border-bottom: none;
  }

  .post-nav-item a:hover {
    color: #222;
    font-weight: bold;
    border-bottom: none;
  }

  .post-nav-item a:active {
    top: 2px;
  }

  .post-nav-item a:before,
  .post-nav-item a:after {
    display: inline-block;
    width: 16px;
    height: 25px;
    vertical-align: top;
    opacity: 0.4;
    background-size: 16px;
  }

  .post-nav-none a:none {
    content: ' ';
    background-size: 8px;
  }

  .post-nav-none a:hover:none {
    opacity: 1;
  }

  .post-nav-prev a:before {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjE4LjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLWxlZnQiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIxOC41MDAwMDAsIDkwLjAwMDAwMCkiPjxwYXRoIGQ9Ik03LjQsMS40IEw2LDAgTC04Ljg4MTc4NDJlLTE2LDYgTDYsMTIgTDcuNCwxMC42IEwyLjgsNiBMNy40LDEuNCBaIiBpZD0iU2hhcGUiLz48L2c+PC9nPjwvZz48L3N2Zz4=") no-repeat 0 50%;
    background-size: 8px;
  }

  .post-nav-prev a:hover:before {
    opacity: 1;
  }

  .post-nav-next {
    text-align: right;
  }

  .post-nav-next a:after {
    content: ' ';
    background: url("data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiA/PjxzdmcgaGVpZ2h0PSIxMnB4IiB2ZXJzaW9uPSIxLjEiIHZpZXdCb3g9IjAgMCA5IDEyIiB3aWR0aD0iOXB4IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnNrZXRjaD0iaHR0cDovL3d3dy5ib2hlbWlhbmNvZGluZy5jb20vc2tldGNoL25zIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHRpdGxlLz48ZGVzYy8+PGRlZnMvPjxnIGZpbGw9Im5vbmUiIGZpbGwtcnVsZT0iZXZlbm9kZCIgaWQ9IlBhZ2UtMSIgc3Ryb2tlPSJub25lIiBzdHJva2Utd2lkdGg9IjEiPjxnIGZpbGw9IiMwMDAwMDAiIGlkPSJDb3JlIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjYwLjAwMDAwMCwgLTkwLjAwMDAwMCkiPjxnIGlkPSJjaGV2cm9uLXJpZ2h0IiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgyNjAuNTAwMDAwLCA5MC4wMDAwMDApIj48cGF0aCBkPSJNMSwwIEwtMC40LDEuNCBMNC4yLDYgTC0wLjQsMTAuNiBMMSwxMiBMNyw2IEwxLDAgWiIgaWQ9IlNoYXBlIi8+PC9nPjwvZz48L2c+PC9zdmc+") no-repeat 100% 50%;
    background-size: 8px;
  }

  .post-nav-next a:hover:after {
    opacity: 1;
  }
</style>



<div class="post-nav">

  
  <div class="post-nav-none post-nav-item">
    <a href=""> </a>
  </div>
  

  
  <div class="post-nav-next post-nav-item">
    
    <a href="/blog/2020/ray/"> 光线追踪-引擎</a>
  </div>
  

</div>

</article>
    </div>

    <footer class="site-footer">
  <div class="container">
    <ul class="social">
  <li><a href="https://twitter.com/penghuailiang" target="_blank"><i class="icon icon-twitter"></i></a></li>
  <li><a href="https://www.zhihu.com/people/huailiangpenguin" target="_blank"><i class="icon icon-zhihu"></i></a></li>
  <li><a href="https://www.facebook.com/profile.php?id=100004290725320" target="_blank"><i class="icon icon-facebook"></i></a></li>
  <li><a href="https://weibo.com/6212299692/profile?topnav=1&wvr=6" target="_blank"><i class="icon icon-sina"></i></a></li>
  <li><a href="https://www.linkedin.com/in/penghuailiang/" target="_blank"><i class="icon icon-linkedin"></i></a></li>
  <li><a href="https://github.com/huailiang" target="_blank"><i class="icon icon-github"></i></a></li>
</ul>
    <p class="txt-medium-gray">
      <small>&copy;2020 All rights reserved. </small>
    </p>
  </div>
</footer>
    <a href="https://github.com/huailiang" target="_blank" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#337ab7; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script>
      $(document).ready(function () {
        $('.sliding-panel-button,.sliding-panel-fade-screen,.sliding-panel-close').on('click touchstart', function (e) {
          $('.sliding-panel-content,.sliding-panel-fade-screen').toggleClass('is-visible');
          e.preventDefault();
        });
      });
    </script>
  </main>
</body>

</html>